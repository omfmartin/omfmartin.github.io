[{"content":"Large Language Models (LLMs) make stuff up. Occasionally, these models will generate content that, while seemingly entirely plausible, is fundamentally incorrect. This phenomenon is known as \u0026ldquo;hallucinations\u0026rdquo;.\nHallucinations can have disastrous consequences. They can spread misinformation, potentially offend users, and even compromise sensitive information. This issue is a major drawback of LLMs and is the reason why many are cautious about employing them in real-world settings.\nIn this article, we will discuss what hallucinations are, explain why they happen, and explore strategies to detect and mitigate them.\nWhat are Hallucinations? Hallucinations can be loosely defined as output that is factually incorrect, unfaithful to the provided information, or nonsensical. Most often, the model will appear quite confident and its response may even look entirely plausible. This makes detecting hallucinations a difficult task. Generally, hallucinations fall into two categories: intrinsic and extrinsic.\nIntrinsic hallucinations happen when the model\u0026rsquo;s output directly contradicts the information it was given. For example:\nPrompt: The capital of France is Paris. What is the capital of France? Response: The capital of France is Lyon.\nExtrinsic hallucinations occur when the model makes statements that cannot be verified or contradicted based on the provided information. For example:\nPrompt: Describe the economic impact of renewable energy sources. Response: According to a 2045 report, renewable energy sources have led to the creation of 5 million new jobs worldwide.\nWhy do LLMs Hallucinate? Hallucinations in language models are thought to be unavoidable. They can occur factors due to a variety of factors, which can be broadly classified into issues related to data, training process, and inference.\nData-Related Causes Inaccuracies in Training Data: Training data is the foundation of LLMs. However, data is not always of the best quality and may be inaccurate or biased. Given the vast amount of data processed, ensuring its accuracy and relevance is challenging. This implies that some of the information learned by the model will lead it to generate misleading or incorrect responses. Out-of-Distribution Queries: LLMs are trained on datasets that cover certain topics, time periods, and types of information. When faced with questions or tasks that lie outside of these boundaries, a language model may generate incorrect answers. For instance, asking about the latest news events will result in answers based on outdated or irrelevant information. Data Utilization Issues: How language models utilize their knowledge base can also contribute to hallucinations. Language models tend to rely on word co-occurrence patterns to generate responses. This reliance might lead to incorrect conclusions, such as identifying Sydney as the capital of Australia due to their more frequent association compared to the correct answer, Canberra. Training-Related Causes Exposure Bias: This issue arises from a discrepancy between how the model is trained and the model\u0026rsquo;s actual use case. During training, the model learns to predict the next token relying on the previous correct sequence from the training data, a method known as \u0026ldquo;teacher forcing.\u0026rdquo; However, during inference, the model relies on its outputs. This can lead to an accumulation of errors or snowball effect. Architectural Limitations: Causal language models generate text by predicting the next token based solely on preceding tokens. This design limits the model\u0026rsquo;s ability to fully grasp contextual nuances, which is believed to contribute to the generation of hallucinated content. Belief Misalignment: LLMs typically undergo a two-stage training process. Initially, they are pretrained to acquire broad knowledge. Subsequently, they are fine-tuned to better align with specific user instructions or prompts, often using techniques like reinforcement learning. This fine-tuning might inadvertently lead the model to prioritize user preferences over accuracy. Inference-Related Causes Randomness in Sampling: The decoding strategies of language models often incorporate some randomness to enhance creativity and output quality, for example by adjusting the temperature parameter to favor less likely choices. This approach can inadvertently increase the likelihood of generating nonsensical content. Softmax Bottleneck: The last layer of an LLM estimates the probability of the next token. The majority of LLMs use a softmax layer, which is known to have a limitation called the softmax bottleneck. This limitation impedes the model\u0026rsquo;s capacity to distinguish among highly probable outcomes effectively. Diluted Attention: With longer input sequences, the efficiency of the modelâ€™s attention mechanism may decline, leading to a weakened focus on essential details and context. This dilution of attention compromises the model\u0026rsquo;s ability to remain coherent and accurate over extended texts. How can we detect hallucinations? Detecting hallucinations presents significant challenges that standard metrics used in Natural Language Processing such as BLEU, ROUGE, or BERTScore fail to address. Not only do these metrics fail to accurately predict hallucinations, but also rely on reference response, which is impractical for widespread applications.\nOne approach for hallucination detection is to employ probabilistic metrics that calculate the likelihood of hallucinations based on token probabilities or entropy measures, for example by estimating perplexity. However, these metrics require access to token probabilities, which may not be available, which limits their adoption.\nA more effective strategy has been the development of model-based metrics. This approach involves utilizing another language model, for example, a state-of-the-art LLM, to assess the likelihood of hallucinatory content. These metrics can be further classified into token-level and sentence-level metrics.\nToken-level metrics attempt to predict for each token if it was hallucinated or not. To implement this strategy, researchers typically generate a synthetic hallucinated dataset by applying perturbations to reference datasets free of hallucinations. A binary language classifier, such as one based on BERT or XLNet, is then trained to discern hallucinated tokens from non-hallucinated ones. Notable projects utilizing this approach are HADES and fairseq-detect-hallucination hallucination detection pipeline.\nOn the other hand, sentence-level metrics evaluate the entire sentence to determine the presence of hallucinations. A prominent method is SelfCheckGPT, a sampling-based technique grounded in the assumption that hallucinations are more likely to occur when the model is uncertain. This method assesses self-consistency by comparing various sampled responses, using standard metrics such as N-gram matching or BERTScore, to estimate the probability of hallucinatory content. One strength of this method is that it does not require any external data, making it easily scalable.\nStrategies for Mitigating Hallucinations Hallucination mitigation strategies aim at reducing the likelihood of an LLM generating hallucinatory content. Below, we explore several techniques, ordered from straightforward adjustments to more complex interventions.\n1. Adjusting Inference Parameters One of the simplest methods to reduce hallucinations involves tweaking the LLM\u0026rsquo;s operational settings. For instance, fine-tuning parameters such as the generation temperature and top-k sampling can regulate the randomness of the LLM\u0026rsquo;s output. Lower temperature or top-k values may yield more cautious and conservative responses, thereby potentially minimizing hallucinatory outputs.\n2. Prompt Engineering Effective prompt engineering is crucial for steering LLMs toward accurate responses. The following general tips are essential for optimizing prompts:\nClarity and Precision: Ensure the prompt is clear about the exact type of information required.\nUncertainty Acknowledgement: Explicitly request that the model should avoid making up information and express uncertainty when applicable, for example by saying \u0026ldquo;I don\u0026rsquo;t know\u0026rdquo;.\nEmphasis on Key Information: Repeating important details within the prompt can help underline the importance of these elements. This helps ensure that they receive due attention in the model\u0026rsquo;s response.\nStrategic Information Placement: Positioning the most crucial information towards the end of the prompt can enhance the model\u0026rsquo;s focus on these key details.\nOutput Format Specification: Directing the model to structure its response in a specific format can help in obtaining information that is organized in a manner that best suits the user\u0026rsquo;s needs.\nFurthermore, more advanced prompt engineering techniques offer additional layers of sophistication:\nMulti-shot Prompting involves providing the model with multiple examples of the desired output. This approach helps the model grasp the context better and align its responses with the expected answer format.\nChain of Thought encourages the model to exhibit its reasoning process step-by-step, offering users insights into the logical progression leading to a given answer.\nSelf-consistency involves generating several responses and selecting the one that demonstrates the highest degree of internal consistency, thereby improving reliability.\nChain of Verification is a technique where the model is used to verify its outputs or make annotations, further enhancing the accuracy and trustworthiness of the information provided.\n3. Retrieval-based Approaches These approaches enhance the model\u0026rsquo;s responses by incorporating external knowledge sources and context to the prompt. Probably the most known technique is Retrieval Augmented Generation (RAG):. This approach encodes external documents as sentence embedding, usually using some transformer-based neural network such as BERT. The resulting embeddings are then stored in a vector database, which can then be efficiently queried using the original prompt (or some variation). The retrieved sentences are then used to enrich the prompt. A popular framework is FAISS. The main drawback of these retrieval-based approaches is that they require maintaining a large and curated database.\n4. LLM Fine-tuning The quality of a model\u0026rsquo;s output directly depends on its training data. Fine-tuning an LLM with updated or more relevant data can address specific weaknesses or gaps in its knowledge base. While retraining from scratch is often impractical, fine-tuning offers a cost-effective alternative for enhancing model accuracy in targeted applications. Techniques such as LoRA help reduce the number of trainable parameters which accelerates the fine-tuning making it a viable option.\nClosing Thought There is a fine line between creativity and hallucinations. On one hand, a model that leans too much towards caution may avoid hallucinations but at the cost of producing responses that are uninteresting or too simplistic. On the other hand, an overly creative model will produce outputs that resemble an interview with Salvador Dali.\nFinding the right balance is key to harnessing the full potential of LLM while minimizing the risks associated with hallucinations. Ultimately, understanding and mitigating hallucinations is not only about preventing errors; it\u0026rsquo;s about building trust between humans and artificial intelligence, ensuring that these tools enhance our decision-making processes, creativity, and access to information.\nBibliography Huang et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. (2023) Ji et al. Survey of Hallucination in Natural Language Generation. (2022) Luo et al. Hallucination Detection and Hallucination Mitigation: An Investigation. (2024) Manakul et al. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (2023) Xu et al. Hallucination is Inevitable: An Innate Limitation of Large Language Models. (2024) Zhou et al. Detecting Hallucinated Content in Conditional Neural Sequence Generation. (2020) ","permalink":"https://omfmartin.com/posts/2024-03-24-llm-hallucination/","summary":"How can we detect and mitigate hallucinations for safer LLM deployment?","title":"Understanding and Mitigating LLM Hallucinations"},{"content":"What is Explainable AI? Every year, artificial intelligence (AI) and machine learning models become more complex, thanks to faster computers, quicker algorithms, and more data. Certain models are now composed of hundreds of billions of parameters. This complexity makes them hard if not impossible to understand.\nThe increase in the number of model parameters between 1954 and 2024. Data from Jaime Sevilla. This is where eXplainable AI (XAI) enters the scene. XAI seeks to open black boxes, providing insights into how and why algorithms make their predictions. By allowing us to explore how data inputs influence model outputs, XAI makes AI more transparent, fosters trust and enables both developers and end-users to understand the model\u0026rsquo;s reasoning.\nThis article offers a short introduction to XAI, highlighting its importance and the methodologies involved. For those seeking a deeper dive into the subject, the resources section at the end of this post offers a selection of books and software tools dedicated to XAI.\nUse Cases for Opening the Black Box Debugging and Improving Models Predictive models make mistakes. Understanding the root causes that lead to these inaccuracies is essential for enhancing model performance. By helping us understand the rationale behind predictions, XAI techniques help us characterize the situations where the model is making errors. This makes XAI a valuable tool for model debugging and contributes to model refinement.\nConsider a scenario involving an animal classification task. XAI techniques, such as saliency maps, can highlight the parts of an image that the classifier considers significant for making a prediction. This analysis might reveal that, in the case of a camel image, the model erroneously concentrates on the sand rather than the camel itself. This would help explain why the model is unable to accurately classify images of camels in other environments.\nBias Detection and Mitigation Predictive models can be biased, leading to social inequalities and harming minority groups. Algorithmic biases usually come from using skewed or incomplete data, or from the models\u0026rsquo; built-in assumptions.\nA notable example of such bias was observed in a commercial algorithm used by the US healthcare system. Specifically, for identical predicted risk scores, Black patients were found to be sicker than their White counterparts. This bias came from the model assuming that higher healthcare costs meant greater healthcare needs. Since less money was spent on the Black population, the algorithm erroneously concluded they were less at risk.\nSimilar instances of algorithmic bias have been documented in the judicial system and job application processes. These examples highlight the need for accountability in AI systems, especially when deployed in areas with significant societal impacts.\nXAI techniques can help identify such biases by contextualizing why models make certain predictions. For instance, in the previously mentioned healthcare example, XAI could demonstrate how being Black lowered the risk score. Therefore, XAI is instrumental not only in interpreting model predictions but also in driving the development of more equitable AI systems.\nHigh Stake Scenarios Erroneous or biased predictions can profoundly impact individuals\u0026rsquo; lives and well-being, particularly when used in high-risk scenarios. For instance, when using predictive algorithms for social scoring systems, individual categorization, facial recognition technologies, law enforcement tools, employment screening processes, and the provision of essential services like healthcare.\nIn practical applications, the explanation can be as important as the prediction itself. For this reason, in high-stakes sectors such as healthcare, finance, and criminal justice, there is a marked preference for employing inherently interpretable models. For example, logistic regression, with its interpretability and simplicity, is often preferred over more complex neural networks within healthcare settings.\nRegulatory Compliance As the deployment of AI becomes more widespread, laws and regulations are starting to require that AI decisions should be transparent and explainable. This shift aims to guarantee that these algorithms operate within ethical boundaries.\nFor instance, the European Union\u0026rsquo;s General Data Protection Regulation (GDPR) mandates significant transparency about the logic involved in automated decisions, particularly in decisions that significantly affect individuals, such as those related to employment, creditworthiness, and legal matters. This regulation has been interpreted by some as a right to explanation. Building on this foundation, the more recent AI Act has introduced specific provisions for \u0026ldquo;high-risk\u0026rdquo; algorithms, with requirements related to \u0026ldquo;transparency and provision of information to users\u0026rdquo;.\nHowever, these regulations face challenges in enforcement and interpretation. The inherent complexity of AI algorithms can make it challenging to provide explanations that are both technically accurate and easily understandable to the general public. Despite these challenges, the push for such regulations highlights the growing concern for transparency in AI applications and underscores the critical need for XAI.\nKnowledge Discovery Using XAI for generating new knowledge is probably the most overlooked application. This capability is particularly interesting given that machine learning models are more apt at identifying complex patterns and trends in data compared to traditional statistical tests such as t-tests or chi-squared tests. This approach was suggested for biomarker discovery.\nDespite its potential, employing XAI for knowledge generation is not without challenges. Some of the most significant issues include:\nSignificance Threshold Ambiguity: Unlike traditional statistical methods, which have well-defined criteria for statistical significance, XAI lacks a standardized framework to determine the significance level of variables. Observational Data Limitations: The majority of data used in machine learning is observational, which is inherently less effective at establishing causal relationships. Overfitting: There exists a risk of models mistaking random noise for significant patterns, potentially leading to incorrect interpretations and conclusions regarding the data. When Can We Get Away Without an Explanation? Not all predictions made by AI should be explained. Generating explanations introduces additional software complexity and computational needs, which translate to financial expenses, and environmental impact. Implementing XAI may be unjustified in certain scenarios, such as:\nLow-Stakes Scenarios: In situations where the consequences of errors are minimal, such as recommendations on a streaming platform. Users typically have little interest in understanding the inner workings of how these suggestions are generated. Well-Understood Problems: For well-established problems like spam filtering in email systems, the explanations for each decision may not be essential because the overall reliability and accuracy of the system are already well understood. Risks of Manipulation: In some cases, providing detailed explanations can unintentionally aid those seeking to exploit the system. For instance, SEO experts might misuse in-depth information about search algorithms to manipulate rankings and undermine the system\u0026rsquo;s integrity. Approaches to Explainaing AI The intricate mathematical details and computations of an algorithm usually don\u0026rsquo;t reveal much about how it makes decisions. This is because the representations learned by the predictive model are typically beyond the human brain\u0026rsquo;s capacity to understand. Because of this, direct approaches, such as disclosing source code or mathematical computations, fall short of providing understanding.\nTo overcome this challenge, XAI techniques either use inherently interpretable models or approximate complex, black-box models with simpler, more understandable ones. XAI is an active field of research. Currently, there is no universal way to explain complex predictive algorithms, and there might never be one.\nOur focus will be on categorizing the techniques used to explain predictive models, highlighting some of the most commonly employed methods.\nStage: Ante-hoc vs Post-hoc Ante-hoc (Intrinsic Interpretability): This approach focuses on using models that are inherently interpretable due to their simple structure and transparent decision-making processes. Examples include linear models, decision trees, and rule-based systems. While these models offer clarity from the outset, their interpretability can diminish as complexity or the number of parameters increases. There is currently research to design high-performing yet inherently interpretable models, however, it has thus far not been very successful. Post-hoc (Extrinsic Interpretability): Post-hoc methods aim to explain decisions made by complex, black-box models after they have been trained. These explanations are not built into the model but are derived from analyzing the model\u0026rsquo;s behavior. This approach is particularly useful for black-box models, such as deep neural networks. Post-hoc explanations strive to be locally faithful, meaning they accurately reflect the model\u0026rsquo;s reasoning for specific instances or decisions. Model: Model-specific vs Model-agnostic Model-Specific: These techniques are only applicable to specific types of models, exploiting their unique architectures to provide explanations. For deep learning models, techniques such as Saliency Maps, DeepLIFT, and Class Activation Maps (CAM) illustrate how different features influence the model\u0026rsquo;s predictions. Model-Agnostic: Offering a flexible alternative, model-agnostic methods do not require knowledge of the model\u0026rsquo;s internal workings, making them applicable across a wide range of AI systems. Techniques like SHAP (SHapley Additive exPlanations) and LIME) (Local Interpretable Model-agnostic Explanations) can be used to explain any model by approximating its decision boundary or output in an interpretable way. Scope: Global vs Local Local Explanations: These are focused on individual predictions or decisions, providing insights into why the model made a particular choice for a specific instance. Tools like SHAP and LIME offer local explanations and show the influence of various features on a single outcome. Global Explanations: In contrast, global explanations seek to show the model\u0026rsquo;s behavior across the entire dataset or in a more general sense. Techniques for achieving global explanations include feature importance rankings and partial dependence plots, which aggregate the effects of features across multiple instances to offer a broader understanding of the model\u0026rsquo;s logic. The Limitations of XAI As AI continues to permeate various aspects of our lives, the imperative for transparency and understandability in AI systems has never been more pronounced. Yet, as with any rapidly evolving domain, XAI is not without its challenges and limitations.\nThe complexity-performance tradeoff The complexity-performance tradeoff suggests that enhancing AI model performance typically requires increasing their complexity. This assumption underlines the perceived necessity of balancing complexity against performance. This perspective, however, merits critical examination. The lottery ticket hypothesis proposes that the efficacy of large models might stem from optimal parameter initialization rather than sheer size. Empirically, simpler models can be shown to achieve comparable performance to their more complex counterparts.\nIndeed, evidence shows that models with fewer parameters, such as Mistral-7B, can achieve performance levels comparable to those of their more complex counterparts, like GPT-3.5, with a fraction of the parameters (7 billion vs. 175 billion). This observation challenges the necessity of overly complex models, advocating for a more measured approach to model design where simplicity does not preclude effectiveness.\nSome researchers argue that non-inherently interpretable models should be avoided altogether in high-stakes scenarios. However, it remains unclear whether this is feasible across all contexts without sacrificing performance.\nExplaining the Explanation For effective implementation of XAI techniques, it is crucial to understand who is the target audience for the explanations. XAI primarily serves technical professionals, such as data scientists and machine learning engineers, who have a deep understanding of statistics.\nHowever, a significant distinction exists between explanations suitable for AI experts and those for non-technical users. Technical details can overwhelm non-technical users, leading to confusion or misinterpretation. For example, someone with limited knowledge of statistics might mistakenly interpret SHAP values as indicating causality, rather than merely correlation.\nAcknowledging the end-user\u0026rsquo;s knowledge, goals, skills, and abilities is paramount when using XAI. Moreover, communication of AI explanations should be aligned with the audience\u0026rsquo;s level of understanding. This is necessary to avoid both the risks of oversimplification, which can lead to misunderstandings, and obfuscation, which can alienate and frustrate users.\nFaithfulness and the Limits of Explanation Post-hoc explanations of complex models are achieved by locally approximating their behavior around a specific data point using a simpler, transparent model. While these approximations offer insights, they fall short of fully capturing the original model\u0026rsquo;s behavior. Furthermore, XAI methods often lack metrics to evaluate the quality of these approximations, raising questions about the faithfulness of such explanations.\nAdditionally, as the complexity of models increases annually, our ability to explain them with accuracy diminishes. This escalating complexity highlights a crucial question: Is there a theoretical limit to our capacity to make AI\u0026rsquo;s decision-making processes transparent? And if such a limit exists, how close are we to reaching it?\nConclusion XAI plays a critical role in addressing the opacity of complex machine learning models, enabling trust in AI-based decisions. AI facilitates identifying biases, debugging models, complying with regulations, and generating new knowledge by explaining model behavior. Numerous open-source resources are available for the easy and efficient implementation of XAI. Nonetheless, it\u0026rsquo;s important to recognize the limitations and challenges associated with XAI and favor simpler models whenever possible.\nResources Books Explanatory Model Analysis Intrepretable Machine Learning Software SHAP LIME DALEX XAITK InterpretML Bibliography Ebers. Regulating Explainable AI in the European Union. An Overview of the Current Legal Framework(s). (2021)\nFrankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. (2018)\nJiang and Senge. On Two XAI Cultures: A Case Study of Non-technical Explanations in Deployed AI System. (2021)\nGunning et al. DARPA\u0026rsquo;s explainable AI (XAI) program: A retrospective. (2021)\nLarson et al. How We Analyzed the COMPAS RecidivismÂ Algorithm. (2016)\nLinardatos et al. Explainable AI: A Review of Machine Learning Interpretability Methods. (2020)\nLungberg and Lee. A Unified Approach to Interpreting Model Predictions. (2017)\nNg et al. The benefits and pitfalls of machine learning for biomarker discovery. (2023)\nObermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. (2019)\nRibeiro et al. \u0026ldquo;Why Should I Trust You?\u0026rdquo;: Explaining the Predictions of Any Classifier. (2016)\nRudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. (2019)\nSarkar. Is explainable AI a race against model complexity? (2022)\nSevilla. Parameter counts in Machine Learning. (2021)\n","permalink":"https://omfmartin.com/posts/2024-02-18-explainable-ai/","summary":"Opening AI\u0026rsquo;s black box to understand how algorithms reason and foster trust.","title":"A Quick Guide to Explainable AI"},{"content":" I am a senior data scientist specializing in healthcare at NTT DATA in Barcelona. I currently focus on developing predictive analytics modules for a clinical trials platform. Additionally, I have implemented a machine learning-powered dashboard to help social care workers anticipate institucionalization in elderly individuals. I also participated in the SmartNation AI Hackathon in Belgium, focusing on structuring free-text clinical data using large language models.\nMy journey into data science began in an unconventional way. Initially studying to become a pharmacist, my curiosity steered me toward structural bioinformatics, statistics, and software development. This shift ultimately led me to pursue a Ph.D. in bioinformatics. Throughout my doctoral work, I explored the impact of aging on the transcriptome of Caenorhabditis elegans, employing various techniques such as graph analysis, bayesian time-series modeling, machine learning, and generalized linear models.\nDuring my career, I also applied machine learning to predict adverse chemotherapy responses utilizing genetic markers, performed kinetic metabolic modeling, and carried out pharmacokinetics simulations.\nAside from my professional pursuits, I have a passion for languages. I am fluent in French, Portuguese, English, Spanish, and Catalan, and can also hold a conversation in Occitan and Russian.\nSoftware zebu: R package for estimating local association measures like chi-squared residuals, Lewontinâ€™s D, pointwise mutual information, and Ducherâ€™s Z. It also incorporates permutation tests for statistical significance assessment coded in C++. Available on CRAN with a detailed usage vignette.\ntrobalÃ¨cte: Currently developing an algorithm in Python for detecting Occitan dialects from text samples using NLP.\nisotela: R package for normalizing and differentially analyzing single-individual RNA-seq data while accounting for tissue size differences. Requires spike-ins.\npoete-mÃ©canique: Reads a random poem aloud when the Raspberry Pi motion sensor detects movement.\nPeer-reviewed Publications Systematic mapping of organism-scale gene-regulatory networks in aging using population asynchrony. Eder, Matthias, Olivier MF Martin, Natasha Oswal, Lucia Sedlackova, CÃ¡tia Moutinho, Andrea Del Carmen-Fabregat, Simon Menendez Bravo, Arnau SebÃ©-PedrÃ³s, Holger Heyn, Nicholas Stroustrup. Cell (2024).\nThe unusual kinetics of lactate dehydrogenase of Schistosoma mansoni and their role in the rapid metabolic switch after penetration of the mammalian host. Bexkens, Michiel L, Olivier MF Martin, Jos M van den Heuvel, Marion GJ Schmitz, Bas Teusink, Barbara M Bakker, Jaap J van Hellemond, Jurgen R Haanstra, Malcolm D Walkinshaw, Aloysius GM Tielens. International Journal for Parasitology (2024)\nNeuronal Temperature Perception Induces Specific Defenses That Enable C. elegans to Cope with the Enhanced Reactivity of Hydrogen Peroxide at High Temperature. Servello, Francesco A, Rute Fernandes, Matthias Eder, Nathan Harris, Olivier MF Martin, Natasha Oswal, Anders Lindberg, et al. eLife 11 (2022)\nA Hierarchical Process Model Links Behavioral Aging and Lifespan in C. elegans.Oswal, Natasha, Olivier MF Martin, Sofia Stroustrup, Monika Anna Matusiak Bruckner, and Nicholas Stroustrup. PLOS Computational Biology 18, no. 9 (2022)\nCaenorhabditis elegans Processes Sensory Information to Choose between Freeloading and Self-Defense Strategies. Schiffer, Jodie A, Francesco A Servello, William R Heath, Francis Raj Gandhi Amrit, Stephanie V Stumbur, Matthias Eder, Olivier MF Martin, et al. eLife 9 (2020)\nImplication of Terminal Residues at Protein-Protein and Protein-DNA Interfaces. Martin, Olivier MF, LoÃ¯c Etheve, Guillaume Launay, and Juliette Martin. PLOS ONE 11, no. 9 (2016).\nAre Ciprofloxacin Dosage Regimens Adequate for Antimicrobial Efficacy and Prevention of Resistance? Pseudomonas aeruginosa Bloodstream Infection in Elderly Patients as a Simulation Case Study. Cazaubon, Yoann, Laurent Bourguignon, Sylvain Goutelle, Olivier MF Martin, Pascal Maire, and Michel Ducher. Fundamental \u0026amp; Clinical Pharmacology 29, no. 6 (2015)\n","permalink":"https://omfmartin.com/about-me/","summary":"I am a senior data scientist specializing in healthcare at NTT DATA in Barcelona. I currently focus on developing predictive analytics modules for a clinical trials platform. Additionally, I have implemented a machine learning-powered dashboard to help social care workers anticipate institucionalization in elderly individuals. I also participated in the SmartNation AI Hackathon in Belgium, focusing on structuring free-text clinical data using large language models.\nMy journey into data science began in an unconventional way.","title":"About Me"}]