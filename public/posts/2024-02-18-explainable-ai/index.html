<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Quick Guide to Explainable AI | Olivier MF Martin</title>
<meta name=keywords content><meta name=description content="Opening AI&rsquo;s black box to understand how algorithms reason and foster trust."><meta name=author content="Olivier MF Martin"><link rel=canonical href=https://omfmartin.com/posts/2024-02-18-explainable-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.c7ada1ec5c18a60093054264541f69ef87055a2648305df1f1d6dd520e8e1f5e.css integrity="sha256-x62h7FwYpgCTBUJkVB9p74cFWiZIMF3x8dbdUg6OH14=" rel="preload stylesheet" as=style><link rel=icon href=https://omfmartin.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://omfmartin.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://omfmartin.com/favicon-32x32.png><link rel=apple-touch-icon href=https://omfmartin.com/apple-touch-icon.png><link rel=mask-icon href=https://omfmartin.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://omfmartin.com/posts/2024-02-18-explainable-ai/><link rel=alternate hreflang=es href=https://omfmartin.com/es/posts/2024-02-18-explainable-ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=/assets/css/extended/style.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-PKR8VY4E8L"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PKR8VY4E8L",{anonymize_ip:!1})}</script><meta property="og:title" content="A Quick Guide to Explainable AI"><meta property="og:description" content="Opening AI&rsquo;s black box to understand how algorithms reason and foster trust."><meta property="og:type" content="article"><meta property="og:url" content="https://omfmartin.com/posts/2024-02-18-explainable-ai/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A Quick Guide to Explainable AI"><meta name=twitter:description content="Opening AI&rsquo;s black box to understand how algorithms reason and foster trust."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://omfmartin.com/posts/"},{"@type":"ListItem","position":2,"name":"A Quick Guide to Explainable AI","item":"https://omfmartin.com/posts/2024-02-18-explainable-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Quick Guide to Explainable AI","name":"A Quick Guide to Explainable AI","description":"Opening AI\u0026rsquo;s black box to understand how algorithms reason and foster trust.","keywords":[],"articleBody":"What is Explainable AI? Every year, artificial intelligence (AI) and machine learning models become more complex, thanks to faster computers, quicker algorithms, and more data. Certain models are now composed of hundreds of billions of parameters. This complexity makes them hard if not impossible to understand.\nThe increase in the number of model parameters between 1954 and 2024. Data from Jaime Sevilla. This is where eXplainable AI (XAI) enters the scene. XAI seeks to open black boxes, providing insights into how and why algorithms make their predictions. By allowing us to explore how data inputs influence model outputs, XAI makes AI more transparent, fosters trust and enables both developers and end-users to understand the model’s reasoning.\nThis article offers a short introduction to XAI, highlighting its importance and the methodologies involved. For those seeking a deeper dive into the subject, the resources section at the end of this post offers a selection of books and software tools dedicated to XAI.\nUse Cases for Opening the Black Box Debugging and Improving Models Predictive models make mistakes. Understanding the root causes that lead to these inaccuracies is essential for enhancing model performance. By helping us understand the rationale behind predictions, XAI techniques help us characterize the situations where the model is making errors. This makes XAI a valuable tool for model debugging and contributes to model refinement.\nConsider a scenario involving an animal classification task. XAI techniques, such as saliency maps, can highlight the parts of an image that the classifier considers significant for making a prediction. This analysis might reveal that, in the case of a camel image, the model erroneously concentrates on the sand rather than the camel itself. This would help explain why the model is unable to accurately classify images of camels in other environments.\nBias Detection and Mitigation Predictive models can be biased, leading to social inequalities and harming minority groups. Algorithmic biases usually come from using skewed or incomplete data, or from the models’ built-in assumptions.\nA notable example of such bias was observed in a commercial algorithm used by the US healthcare system. Specifically, for identical predicted risk scores, Black patients were found to be sicker than their White counterparts. This bias came from the model assuming that higher healthcare costs meant greater healthcare needs. Since less money was spent on the Black population, the algorithm erroneously concluded they were less at risk.\nSimilar instances of algorithmic bias have been documented in the judicial system and job application processes. These examples highlight the need for accountability in AI systems, especially when deployed in areas with significant societal impacts.\nXAI techniques can help identify such biases by contextualizing why models make certain predictions. For instance, in the previously mentioned healthcare example, XAI could demonstrate how being Black lowered the risk score. Therefore, XAI is instrumental not only in interpreting model predictions but also in driving the development of more equitable AI systems.\nHigh Stake Scenarios Erroneous or biased predictions can profoundly impact individuals’ lives and well-being, particularly when used in high-risk scenarios. For instance, when using predictive algorithms for social scoring systems, individual categorization, facial recognition technologies, law enforcement tools, employment screening processes, and the provision of essential services like healthcare.\nIn practical applications, the explanation can be as important as the prediction itself. For this reason, in high-stakes sectors such as healthcare, finance, and criminal justice, there is a marked preference for employing inherently interpretable models. For example, logistic regression, with its interpretability and simplicity, is often preferred over more complex neural networks within healthcare settings.\nRegulatory Compliance As the deployment of AI becomes more widespread, laws and regulations are starting to require that AI decisions should be transparent and explainable. This shift aims to guarantee that these algorithms operate within ethical boundaries.\nFor instance, the European Union’s General Data Protection Regulation (GDPR) mandates significant transparency about the logic involved in automated decisions, particularly in decisions that significantly affect individuals, such as those related to employment, creditworthiness, and legal matters. This regulation has been interpreted by some as a right to explanation. Building on this foundation, the more recent AI Act has introduced specific provisions for “high-risk” algorithms, with requirements related to “transparency and provision of information to users”.\nHowever, these regulations face challenges in enforcement and interpretation. The inherent complexity of AI algorithms can make it challenging to provide explanations that are both technically accurate and easily understandable to the general public. Despite these challenges, the push for such regulations highlights the growing concern for transparency in AI applications and underscores the critical need for XAI.\nKnowledge Discovery Using XAI for generating new knowledge is probably the most overlooked application. This capability is particularly interesting given that machine learning models are more apt at identifying complex patterns and trends in data compared to traditional statistical tests such as t-tests or chi-squared tests. This approach was suggested for biomarker discovery.\nDespite its potential, employing XAI for knowledge generation is not without challenges. Some of the most significant issues include:\nSignificance Threshold Ambiguity: Unlike traditional statistical methods, which have well-defined criteria for statistical significance, XAI lacks a standardized framework to determine the significance level of variables. Observational Data Limitations: The majority of data used in machine learning is observational, which is inherently less effective at establishing causal relationships. Overfitting: There exists a risk of models mistaking random noise for significant patterns, potentially leading to incorrect interpretations and conclusions regarding the data. When Can We Get Away Without an Explanation? Not all predictions made by AI should be explained. Generating explanations introduces additional software complexity and computational needs, which translate to financial expenses, and environmental impact. Implementing XAI may be unjustified in certain scenarios, such as:\nLow-Stakes Scenarios: In situations where the consequences of errors are minimal, such as recommendations on a streaming platform. Users typically have little interest in understanding the inner workings of how these suggestions are generated. Well-Understood Problems: For well-established problems like spam filtering in email systems, the explanations for each decision may not be essential because the overall reliability and accuracy of the system are already well understood. Risks of Manipulation: In some cases, providing detailed explanations can unintentionally aid those seeking to exploit the system. For instance, SEO experts might misuse in-depth information about search algorithms to manipulate rankings and undermine the system’s integrity. Approaches to Explainaing AI The intricate mathematical details and computations of an algorithm usually don’t reveal much about how it makes decisions. This is because the representations learned by the predictive model are typically beyond the human brain’s capacity to understand. Because of this, direct approaches, such as disclosing source code or mathematical computations, fall short of providing understanding.\nTo overcome this challenge, XAI techniques either use inherently interpretable models or approximate complex, black-box models with simpler, more understandable ones. XAI is an active field of research. Currently, there is no universal way to explain complex predictive algorithms, and there might never be one.\nOur focus will be on categorizing the techniques used to explain predictive models, highlighting some of the most commonly employed methods.\nStage: Ante-hoc vs Post-hoc Ante-hoc (Intrinsic Interpretability): This approach focuses on using models that are inherently interpretable due to their simple structure and transparent decision-making processes. Examples include linear models, decision trees, and rule-based systems. While these models offer clarity from the outset, their interpretability can diminish as complexity or the number of parameters increases. There is currently research to design high-performing yet inherently interpretable models, however, it has thus far not been very successful. Post-hoc (Extrinsic Interpretability): Post-hoc methods aim to explain decisions made by complex, black-box models after they have been trained. These explanations are not built into the model but are derived from analyzing the model’s behavior. This approach is particularly useful for black-box models, such as deep neural networks. Post-hoc explanations strive to be locally faithful, meaning they accurately reflect the model’s reasoning for specific instances or decisions. Model: Model-specific vs Model-agnostic Model-Specific: These techniques are only applicable to specific types of models, exploiting their unique architectures to provide explanations. For deep learning models, techniques such as Saliency Maps, DeepLIFT, and Class Activation Maps (CAM) illustrate how different features influence the model’s predictions. Model-Agnostic: Offering a flexible alternative, model-agnostic methods do not require knowledge of the model’s internal workings, making them applicable across a wide range of AI systems. Techniques like SHAP (SHapley Additive exPlanations) and LIME) (Local Interpretable Model-agnostic Explanations) can be used to explain any model by approximating its decision boundary or output in an interpretable way. Scope: Global vs Local Local Explanations: These are focused on individual predictions or decisions, providing insights into why the model made a particular choice for a specific instance. Tools like SHAP and LIME offer local explanations and show the influence of various features on a single outcome. Global Explanations: In contrast, global explanations seek to show the model’s behavior across the entire dataset or in a more general sense. Techniques for achieving global explanations include feature importance rankings and partial dependence plots, which aggregate the effects of features across multiple instances to offer a broader understanding of the model’s logic. The Limitations of XAI As AI continues to permeate various aspects of our lives, the imperative for transparency and understandability in AI systems has never been more pronounced. Yet, as with any rapidly evolving domain, XAI is not without its challenges and limitations.\nThe complexity-performance tradeoff The complexity-performance tradeoff suggests that enhancing AI model performance typically requires increasing their complexity. This assumption underlines the perceived necessity of balancing complexity against performance. This perspective, however, merits critical examination. The lottery ticket hypothesis proposes that the efficacy of large models might stem from optimal parameter initialization rather than sheer size. Empirically, simpler models can be shown to achieve comparable performance to their more complex counterparts.\nIndeed, evidence shows that models with fewer parameters, such as Mistral-7B, can achieve performance levels comparable to those of their more complex counterparts, like GPT-3.5, with a fraction of the parameters (7 billion vs. 175 billion). This observation challenges the necessity of overly complex models, advocating for a more measured approach to model design where simplicity does not preclude effectiveness.\nSome researchers argue that non-inherently interpretable models should be avoided altogether in high-stakes scenarios. However, it remains unclear whether this is feasible across all contexts without sacrificing performance.\nExplaining the Explanation For effective implementation of XAI techniques, it is crucial to understand who is the target audience for the explanations. XAI primarily serves technical professionals, such as data scientists and machine learning engineers, who have a deep understanding of statistics.\nHowever, a significant distinction exists between explanations suitable for AI experts and those for non-technical users. Technical details can overwhelm non-technical users, leading to confusion or misinterpretation. For example, someone with limited knowledge of statistics might mistakenly interpret SHAP values as indicating causality, rather than merely correlation.\nAcknowledging the end-user’s knowledge, goals, skills, and abilities is paramount when using XAI. Moreover, communication of AI explanations should be aligned with the audience’s level of understanding. This is necessary to avoid both the risks of oversimplification, which can lead to misunderstandings, and obfuscation, which can alienate and frustrate users.\nFaithfulness and the Limits of Explanation Post-hoc explanations of complex models are achieved by locally approximating their behavior around a specific data point using a simpler, transparent model. While these approximations offer insights, they fall short of fully capturing the original model’s behavior. Furthermore, XAI methods often lack metrics to evaluate the quality of these approximations, raising questions about the faithfulness of such explanations.\nAdditionally, as the complexity of models increases annually, our ability to explain them with accuracy diminishes. This escalating complexity highlights a crucial question: Is there a theoretical limit to our capacity to make AI’s decision-making processes transparent? And if such a limit exists, how close are we to reaching it?\nConclusion XAI plays a critical role in addressing the opacity of complex machine learning models, enabling trust in AI-based decisions. AI facilitates identifying biases, debugging models, complying with regulations, and generating new knowledge by explaining model behavior. Numerous open-source resources are available for the easy and efficient implementation of XAI. Nonetheless, it’s important to recognize the limitations and challenges associated with XAI and favor simpler models whenever possible.\nResources Books Explanatory Model Analysis Intrepretable Machine Learning Software SHAP LIME DALEX XAITK InterpretML Bibliography Ebers. Regulating Explainable AI in the European Union. An Overview of the Current Legal Framework(s). (2021)\nFrankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. (2018)\nJiang and Senge. On Two XAI Cultures: A Case Study of Non-technical Explanations in Deployed AI System. (2021)\nGunning et al. DARPA’s explainable AI (XAI) program: A retrospective. (2021)\nLarson et al. How We Analyzed the COMPAS Recidivism Algorithm. (2016)\nLinardatos et al. Explainable AI: A Review of Machine Learning Interpretability Methods. (2020)\nLungberg and Lee. A Unified Approach to Interpreting Model Predictions. (2017)\nNg et al. The benefits and pitfalls of machine learning for biomarker discovery. (2023)\nObermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. (2019)\nRibeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. (2016)\nRudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. (2019)\nSarkar. Is explainable AI a race against model complexity? (2022)\nSevilla. Parameter counts in Machine Learning. (2021)\n","wordCount":"2232","inLanguage":"en","datePublished":"2024-02-18T00:00:00Z","dateModified":"2024-02-18T00:00:00Z","author":{"@type":"Person","name":"Olivier MF Martin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://omfmartin.com/posts/2024-02-18-explainable-ai/"},"publisher":{"@type":"Organization","name":"Olivier MF Martin","logo":{"@type":"ImageObject","url":"https://omfmartin.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://omfmartin.com accesskey=h title="Olivier MF Martin (Alt + H)">Olivier MF Martin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://omfmartin.com/es/ title=Español aria-label=Español>Español</a></li></ul></div></div><ul id=menu><li><a href=https://omfmartin.com/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://omfmartin.com/about-me title="About Me"><span>About Me</span></a></li><li><a href=https://omfmartin.com/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">A Quick Guide to Explainable AI</h1><div class=post-meta><span title='2024-02-18 00:00:00 +0000 UTC'>February 18, 2024</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Olivier MF Martin&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://omfmartin.com/es/posts/2024-02-18-explainable-ai/>Español</a></li></ul></div></header><div class=post-content><h2 id=what-is-explainable-ai>What is Explainable AI?<a hidden class=anchor aria-hidden=true href=#what-is-explainable-ai>#</a></h2><p>Every year, artificial intelligence (AI) and machine learning models become more complex, thanks to faster computers, quicker algorithms, and more data. Certain models are now composed of hundreds of billions of parameters. This complexity makes them hard if not impossible to understand.</p><figure><img style=margin:auto src=increasing_number_of_parameters.png alt="The increasing number of AI model parameters between 1954 and 2024."><figcaption>The increase in the number of model parameters between 1954 and 2024. Data from <a href=https://towardsdatascience.com/parameter-counts-in-machine-learning-a312dc4753d0>Jaime Sevilla</a>.</figcaption></figure><p>This is where eXplainable AI (XAI) enters the scene. XAI seeks to open black boxes, providing insights into how and why algorithms make their predictions. By allowing us to explore how data inputs influence model outputs, XAI makes AI more transparent, fosters trust and enables both developers and end-users to understand the model&rsquo;s reasoning.</p><p>This article offers a short introduction to XAI, highlighting its importance and the methodologies involved. For those seeking a deeper dive into the subject, the resources section at the end of this post offers a selection of books and software tools dedicated to XAI.</p><h2 id=use-cases-for-opening-the-black-box>Use Cases for Opening the Black Box<a hidden class=anchor aria-hidden=true href=#use-cases-for-opening-the-black-box>#</a></h2><p><img loading=lazy src=opening_the_black_box.png alt="A person stands in a dimly lit room, their silhouette outlined by the soft glow emanating from an open, mysterious black box they hold in their hands."></p><h3 id=debugging-and-improving-models>Debugging and Improving Models<a hidden class=anchor aria-hidden=true href=#debugging-and-improving-models>#</a></h3><p>Predictive models make mistakes. Understanding the root causes that lead to these inaccuracies is essential for enhancing model performance. By helping us understand the rationale behind predictions, XAI techniques help us characterize the situations where the model is making errors. This makes XAI a valuable tool for model debugging and contributes to model refinement.</p><p>Consider a scenario involving an animal classification task. XAI techniques, such as <a href=https://en.wikipedia.org/wiki/Saliency_map>saliency maps</a>, can highlight the parts of an image that the classifier considers significant for making a prediction. This analysis might reveal that, in the case of a camel image, the model erroneously concentrates on the sand rather than the camel itself. This would help explain why the model is unable to accurately classify images of camels in other environments.</p><h3 id=bias-detection-and-mitigation>Bias Detection and Mitigation<a hidden class=anchor aria-hidden=true href=#bias-detection-and-mitigation>#</a></h3><p>Predictive models can be biased, leading to social inequalities and harming minority groups. Algorithmic biases usually come from using skewed or incomplete data, or from the models&rsquo; built-in assumptions.</p><p>A notable example of such bias was observed in a commercial <a href=https://www.science.org/doi/10.1126/science.aax2342#supplementary-materials>algorithm used by the US healthcare system</a>. Specifically, for identical predicted risk scores, Black patients were found to be sicker than their White counterparts. This bias came from the model assuming that higher healthcare costs meant greater healthcare needs. Since less money was spent on the Black population, the algorithm erroneously concluded they were less at risk.</p><p>Similar instances of algorithmic bias have been documented in the <a href=https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm>judicial system</a> and <a href=https://www.bbc.com/news/technology-45809919>job application processes</a>. These examples highlight the need for accountability in AI systems, especially when deployed in areas with significant societal impacts.</p><p>XAI techniques can help identify such biases by contextualizing why models make certain predictions. For instance, in the previously mentioned healthcare example, XAI could demonstrate how being Black lowered the risk score. Therefore, XAI is instrumental not only in interpreting model predictions but also in driving the development of more equitable AI systems.</p><h2 id=high-stake-scenarios>High Stake Scenarios<a hidden class=anchor aria-hidden=true href=#high-stake-scenarios>#</a></h2><p>Erroneous or biased predictions can profoundly impact individuals&rsquo; lives and well-being, particularly when used in high-risk scenarios. For instance, when using predictive algorithms for social scoring systems, individual categorization, facial recognition technologies, law enforcement tools, employment screening processes, and the provision of essential services like healthcare.</p><p>In practical applications, the explanation can be as important as the prediction itself. For this reason, in high-stakes sectors such as healthcare, finance, and criminal justice, there is a marked preference for employing inherently interpretable models. For example, logistic regression, with its interpretability and simplicity, is often preferred over more complex neural networks within healthcare settings.</p><h3 id=regulatory-compliance>Regulatory Compliance<a hidden class=anchor aria-hidden=true href=#regulatory-compliance>#</a></h3><p>As the deployment of AI becomes more widespread, laws and regulations are starting to require that AI decisions should be transparent and explainable. This shift aims to guarantee that these algorithms operate within ethical boundaries.</p><p>For instance, the European Union&rsquo;s General Data Protection Regulation (GDPR) mandates significant transparency about the logic involved in automated decisions, particularly in decisions that significantly affect individuals, such as those related to employment, creditworthiness, and legal matters. This regulation has been interpreted by some as a <em>right to explanation</em>. Building on this foundation, the more recent AI Act has introduced specific provisions for &ldquo;high-risk&rdquo; algorithms, with requirements related to &ldquo;transparency and provision of information to users&rdquo;.</p><p>However, these regulations face challenges in enforcement and interpretation. The inherent complexity of AI algorithms can make it challenging to provide explanations that are both technically accurate and easily understandable to the general public. Despite these challenges, the push for such regulations highlights the growing concern for transparency in AI applications and underscores the critical need for XAI.</p><h3 id=knowledge-discovery>Knowledge Discovery<a hidden class=anchor aria-hidden=true href=#knowledge-discovery>#</a></h3><p>Using XAI for generating new knowledge is probably the most overlooked application. This capability is particularly interesting given that machine learning models are more apt at identifying complex patterns and trends in data compared to traditional statistical tests such as t-tests or chi-squared tests. This approach was suggested for <a href=https://doi.org/10.1007/s00441-023-03816-z>biomarker discovery</a>.</p><p>Despite its potential, employing XAI for knowledge generation is not without challenges. Some of the most significant issues include:</p><ul><li><strong>Significance Threshold Ambiguity</strong>: Unlike traditional statistical methods, which have well-defined criteria for statistical significance, XAI lacks a standardized framework to determine the significance level of variables.</li><li><strong>Observational Data Limitations</strong>: The majority of data used in machine learning is observational, which is inherently less effective at establishing causal relationships.</li><li><strong>Overfitting</strong>: There exists a risk of models mistaking random noise for significant patterns, potentially leading to incorrect interpretations and conclusions regarding the data.</li></ul><h3 id=when-can-we-get-away-without-an-explanation>When Can We Get Away Without an Explanation?<a hidden class=anchor aria-hidden=true href=#when-can-we-get-away-without-an-explanation>#</a></h3><p>Not all predictions made by AI should be explained. Generating explanations introduces additional software complexity and computational needs, which translate to financial expenses, and environmental impact. Implementing XAI may be unjustified in certain scenarios, such as:</p><ul><li><strong>Low-Stakes Scenarios</strong>: In situations where the consequences of errors are minimal, such as recommendations on a streaming platform. Users typically have little interest in understanding the inner workings of how these suggestions are generated.</li><li><strong>Well-Understood Problems</strong>: For well-established problems like spam filtering in email systems, the explanations for each decision may not be essential because the overall reliability and accuracy of the system are already well understood.</li><li><strong>Risks of Manipulation</strong>: In some cases, providing detailed explanations can unintentionally aid those seeking to exploit the system. For instance, SEO experts might misuse in-depth information about search algorithms to manipulate rankings and undermine the system&rsquo;s integrity.</li></ul><h2 id=approaches-to-explainaing-ai>Approaches to Explainaing AI<a hidden class=anchor aria-hidden=true href=#approaches-to-explainaing-ai>#</a></h2><p>The intricate mathematical details and computations of an algorithm usually don&rsquo;t reveal much about how it makes decisions. This is because the representations learned by the predictive model are typically beyond the human brain&rsquo;s capacity to understand. Because of this, direct approaches, such as disclosing source code or mathematical computations, fall short of providing understanding.</p><p>To overcome this challenge, XAI techniques either use inherently interpretable models or approximate complex, black-box models with simpler, more understandable ones. XAI is an active field of research. Currently, there is no universal way to explain complex predictive algorithms, and there might never be one.</p><p>Our focus will be on categorizing the techniques used to explain predictive models, highlighting some of the most commonly employed methods.</p><p><img loading=lazy src=classification_of_xai.png alt="An abstract visual representation of a classification of XAI techniques."></p><h3 id=stage-ante-hoc-vs-post-hoc>Stage: Ante-hoc vs Post-hoc<a hidden class=anchor aria-hidden=true href=#stage-ante-hoc-vs-post-hoc>#</a></h3><ul><li><strong>Ante-hoc (Intrinsic Interpretability)</strong>: This approach focuses on using models that are inherently interpretable due to their simple structure and transparent decision-making processes. Examples include linear models, decision trees, and rule-based systems. While these models offer clarity from the outset, their interpretability can diminish as complexity or the number of parameters increases. There is currently research to design high-performing yet inherently interpretable models, however, it has thus far not been very successful.</li><li><strong>Post-hoc (Extrinsic Interpretability)</strong>: Post-hoc methods aim to explain decisions made by complex, black-box models after they have been trained. These explanations are not built into the model but are derived from analyzing the model&rsquo;s behavior. This approach is particularly useful for black-box models, such as deep neural networks. Post-hoc explanations strive to be locally faithful, meaning they accurately reflect the model&rsquo;s reasoning for specific instances or decisions.</li></ul><h3 id=model-model-specific-vs-model-agnostic>Model: Model-specific vs Model-agnostic<a hidden class=anchor aria-hidden=true href=#model-model-specific-vs-model-agnostic>#</a></h3><ul><li><strong>Model-Specific</strong>: These techniques are only applicable to specific types of models, exploiting their unique architectures to provide explanations. For deep learning models, techniques such as Saliency Maps, DeepLIFT, and Class Activation Maps (CAM) illustrate how different features influence the model&rsquo;s predictions.</li><li><strong>Model-Agnostic</strong>: Offering a flexible alternative, model-agnostic methods do not require knowledge of the model&rsquo;s internal workings, making them applicable across a wide range of AI systems. Techniques like <a href=https://doi.org/10.48550/arXiv.1705.07874>SHAP</a> (SHapley Additive exPlanations) and <a href=https://github.com/marcotcr/lime>LIME</a>) (Local Interpretable Model-agnostic Explanations) can be used to explain any model by approximating its decision boundary or output in an interpretable way.</li></ul><h3 id=scope-global-vs-local>Scope: Global vs Local<a hidden class=anchor aria-hidden=true href=#scope-global-vs-local>#</a></h3><ul><li><strong>Local Explanations</strong>: These are focused on individual predictions or decisions, providing insights into why the model made a particular choice for a specific instance. Tools like SHAP and LIME offer local explanations and show the influence of various features on a single outcome.</li><li><strong>Global Explanations</strong>: In contrast, global explanations seek to show the model&rsquo;s behavior across the entire dataset or in a more general sense. Techniques for achieving global explanations include feature importance rankings and partial dependence plots, which aggregate the effects of features across multiple instances to offer a broader understanding of the model&rsquo;s logic.</li></ul><h2 id=the-limitations-of-xai>The Limitations of XAI<a hidden class=anchor aria-hidden=true href=#the-limitations-of-xai>#</a></h2><p>As AI continues to permeate various aspects of our lives, the imperative for transparency and understandability in AI systems has never been more pronounced. Yet, as with any rapidly evolving domain, XAI is not without its challenges and limitations.</p><p><img loading=lazy src=complex_ai.png alt="The image depicts a person sitting in front of a massive, complex machine, composed of countless gears, levers, and screens, symbolizing a complex AI."></p><h3 id=the-complexity-performance-tradeoff>The complexity-performance tradeoff<a hidden class=anchor aria-hidden=true href=#the-complexity-performance-tradeoff>#</a></h3><p>The complexity-performance tradeoff suggests that enhancing AI model performance typically requires increasing their complexity. This assumption underlines the perceived necessity of balancing complexity against performance. This perspective, however, merits critical examination. The <a href=https://doi.org/10.48550/arXiv.1803.03635>lottery ticket hypothesis</a> proposes that the efficacy of large models might stem from optimal parameter initialization rather than sheer size. Empirically, simpler models can be shown to achieve comparable performance to their more complex counterparts.</p><p>Indeed, evidence shows that models with fewer parameters, such as Mistral-7B, can achieve performance levels comparable to those of their more complex counterparts, like GPT-3.5, with a fraction of the parameters (7 billion vs. 175 billion). This observation challenges the necessity of overly complex models, advocating for a more measured approach to model design where simplicity does not preclude effectiveness.</p><p><a href=https://doi.org/10.1038%2Fs42256-019-0048-x>Some researchers</a> argue that non-inherently interpretable models should be avoided altogether in high-stakes scenarios. However, it remains unclear whether this is feasible across all contexts without sacrificing performance.</p><h3 id=explaining-the-explanation>Explaining the Explanation<a hidden class=anchor aria-hidden=true href=#explaining-the-explanation>#</a></h3><p>For effective implementation of XAI techniques, it is crucial to understand who is the target audience for the explanations. XAI primarily serves technical professionals, such as data scientists and machine learning engineers, who have a deep understanding of statistics.</p><p>However, a significant distinction exists between explanations suitable for AI experts and those for non-technical users. Technical details can overwhelm non-technical users, leading to confusion or misinterpretation. For example, someone with limited knowledge of statistics might mistakenly interpret SHAP values as indicating causality, rather than merely correlation.</p><p>Acknowledging the end-user&rsquo;s knowledge, goals, skills, and abilities is paramount when using XAI. Moreover, communication of AI explanations should be aligned with the audience&rsquo;s level of understanding. This is necessary to avoid both the risks of oversimplification, which can lead to misunderstandings, and obfuscation, which can alienate and frustrate users.</p><h3 id=faithfulness-and-the-limits-of-explanation>Faithfulness and the Limits of Explanation<a hidden class=anchor aria-hidden=true href=#faithfulness-and-the-limits-of-explanation>#</a></h3><p>Post-hoc explanations of complex models are achieved by locally approximating their behavior around a specific data point using a simpler, transparent model. While these approximations offer insights, they fall short of fully capturing the original model&rsquo;s behavior. Furthermore, XAI methods often lack metrics to evaluate the quality of these approximations, raising questions about the faithfulness of such explanations.</p><p>Additionally, as the complexity of models increases annually, our ability to explain them with accuracy diminishes. This escalating complexity highlights a crucial question: Is there a theoretical limit to our capacity to make AI&rsquo;s decision-making processes transparent? And if such a limit exists, how close are we to reaching it?</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>XAI plays a critical role in addressing the opacity of complex machine learning models, enabling trust in AI-based decisions. AI facilitates identifying biases, debugging models, complying with regulations, and generating new knowledge by explaining model behavior. Numerous open-source resources are available for the easy and efficient implementation of XAI. Nonetheless, it&rsquo;s important to recognize the limitations and challenges associated with XAI and favor simpler models whenever possible.</p><h2 id=resources>Resources<a hidden class=anchor aria-hidden=true href=#resources>#</a></h2><h3 id=books>Books<a hidden class=anchor aria-hidden=true href=#books>#</a></h3><ul><li><a href=https://ema.drwhy.ai/>Explanatory Model Analysis</a></li><li><a href=https://christophm.github.io/interpretable-ml-book/>Intrepretable Machine Learning</a></li></ul><h3 id=software>Software<a hidden class=anchor aria-hidden=true href=#software>#</a></h3><ul><li><a href=https://shap.readthedocs.io/en/latest/>SHAP</a></li><li><a href=https://github.com/marcotcr/lime>LIME</a></li><li><a href=https://github.com/ModelOriented/DALEX>DALEX</a></li><li><a href=https://xaitk.org/>XAITK</a></li><li><a href=https://interpret.ml/>InterpretML</a></li></ul><h2 id=bibliography>Bibliography<a hidden class=anchor aria-hidden=true href=#bibliography>#</a></h2><ul><li><p><a href=https://dx.doi.org/10.2139/ssrn.3901732>Ebers. Regulating Explainable AI in the European Union. An Overview of the Current Legal Framework(s). (2021)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.1803.03635>Frankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. (2018)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.2112.01016>Jiang and Senge. On Two XAI Cultures: A Case Study of Non-technical Explanations in Deployed AI System. (2021)</a></p></li><li><p><a href=https://doi.org/10.1002/ail2.61>Gunning et al. DARPA&rsquo;s explainable AI (XAI) program: A retrospective. (2021)</a></p></li><li><p><a href=https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm>Larson et al. How We Analyzed the COMPAS Recidivism Algorithm. (2016)</a></p></li><li><p><a href=https://doi.org/10.3390/e23010018>Linardatos et al. Explainable AI: A Review of Machine Learning Interpretability Methods. (2020)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.1705.07874>Lungberg and Lee. A Unified Approach to Interpreting Model Predictions. (2017)</a></p></li><li><p><a href=https://doi.org/10.1007/s00441-023-03816-z>Ng et al. The benefits and pitfalls of machine learning for biomarker discovery. (2023)</a></p></li><li><p><a href=https://doi.org/10.1126/science.aax2342>Obermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. (2019)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.1602.04938>Ribeiro et al. &ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier. (2016)</a></p></li><li><p><a href=https://doi.org/10.1038%2Fs42256-019-0048-x>Rudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. (2019)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.2205.10119>Sarkar. Is explainable AI a race against model complexity? (2022)</a></p></li><li><p><a href=https://towardsdatascience.com/parameter-counts-in-machine-learning-a312dc4753d0>Sevilla. Parameter counts in Machine Learning. (2021)</a></p></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://omfmartin.com/posts/2024-03-24-llm-hallucination/><span class=title>« Prev</span><br><span>Understanding and Mitigating LLM Hallucinations</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://omfmartin.com>Olivier MF Martin</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>