<!doctype html><html lang=es dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Entender y Mitigar las Alucinaciones de los LLMs | Olivier MF Martin</title>
<meta name=keywords content><meta name=description content="¿Cómo podemos detectar y mitigar las alucinaciones para un despliegue más seguro de los LLMs?"><meta name=author content="Olivier MF Martin"><link rel=canonical href=https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/><link crossorigin=anonymous href=/assets/css/stylesheet.c7ada1ec5c18a60093054264541f69ef87055a2648305df1f1d6dd520e8e1f5e.css integrity="sha256-x62h7FwYpgCTBUJkVB9p74cFWiZIMF3x8dbdUg6OH14=" rel="preload stylesheet" as=style><link rel=icon href=https://omfmartin.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://omfmartin.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://omfmartin.com/favicon-32x32.png><link rel=apple-touch-icon href=https://omfmartin.com/apple-touch-icon.png><link rel=mask-icon href=https://omfmartin.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://omfmartin.com/posts/2024-03-24-llm-hallucination/><link rel=alternate hreflang=es href=https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=/assets/css/extended/style.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-PKR8VY4E8L"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PKR8VY4E8L",{anonymize_ip:!1})}</script><meta property="og:title" content="Entender y Mitigar las Alucinaciones de los LLMs"><meta property="og:description" content="¿Cómo podemos detectar y mitigar las alucinaciones para un despliegue más seguro de los LLMs?"><meta property="og:type" content="article"><meta property="og:url" content="https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-24T00:00:00+00:00"><meta property="article:modified_time" content="2024-03-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Entender y Mitigar las Alucinaciones de los LLMs"><meta name=twitter:description content="¿Cómo podemos detectar y mitigar las alucinaciones para un despliegue más seguro de los LLMs?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://omfmartin.com/es/posts/"},{"@type":"ListItem","position":2,"name":"Entender y Mitigar las Alucinaciones de los LLMs","item":"https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Entender y Mitigar las Alucinaciones de los LLMs","name":"Entender y Mitigar las Alucinaciones de los LLMs","description":"¿Cómo podemos detectar y mitigar las alucinaciones para un despliegue más seguro de los LLMs?","keywords":[],"articleBody":"Los Modelos de Lenguaje de Gran Tamaño (LLMs) inventan cosas. Ocasionalmente, estos modelos generarán contenido que, aunque parezca totalmente plausible, es fundamentalmente incorrecto. Este fenómeno se conoce como “alucinaciones”.\nLas alucinaciones pueden tener consecuencias desastrosas. Pueden difundir información errónea, potencialmente ofender a usuarios, e incluso comprometer información sensible. Este problema es un gran inconveniente de los LLMs y es la razón por la que muchos son cautelosos a la hora de emplearlos en entornos del mundo real.\nEn este artículo, discutiremos qué son las alucinaciones, explicaremos por qué ocurren y exploraremos estrategias para detectarlas y mitigarlas.\n¿Qué son las Alucinaciones? Las alucinaciones pueden definirse de manera general como respuestas que son factualmente incorrectas, infieles a la información proporcionada, o sin sentido. La mayoría de las veces, el modelo parecerá bastante seguro y su respuesta puede incluso parecer totalmente plausible. Esto hace que detectar alucinaciones sea una tarea difícil. Generalmente, las alucinaciones se dividen en dos categorías: intrínsecas y extrínsecas.\nLas alucinaciones intrínsecas ocurren cuando la respuesta del modelo contradice directamente la información que se le dio. Por ejemplo:\nPrompt: La capital de Francia es París. ¿Cuál es la capital de Francia? Respuesta: La capital de Francia es Lyon.\nLas alucinaciones extrínsecas se producen cuando el modelo realiza afirmaciones que no pueden ser verificadas ni contradichas con la información proporcionada. Por ejemplo:\nPrompt: Describe el impacto económico de las fuentes de energía renovable. Respuesta: Según un informe de 2045, las fuentes de energía renovable han llevado a la creación de 5 millones de nuevos empleos en todo el mundo.\n¿Por qué los LLMs Alucinan? Se considera que las alucinaciones en los modelos de lenguaje son inevitables. Pueden ocurrir debido a una variedad de factores, los cuales se pueden clasificar de manera general en problemas relacionados con los datos, el proceso de entrenamiento y la inferencia.\nCausas Relacionadas con los Datos Inexactitudes en los Datos de Entrenamiento: Los datos de entrenamiento son la base de los LLMs. Sin embargo, los datos no siempre son de la mejor calidad y pueden ser inexactos o sesgados. Dada la vasta cantidad de datos procesados, asegurar su precisión y relevancia es un desafío. Esto implica que parte de la información aprendida por el modelo lo llevará a generar respuestas engañosas o incorrectas. Consultas Fuera de Distribución: Los LLMs se entrenan en conjuntos de datos que cubren ciertos temas, períodos de tiempo y tipos de información. Cuando se enfrentan a preguntas o tareas que se encuentran fuera de estos límites, un modelo de lenguaje puede generar respuestas incorrectas. Por ejemplo, preguntar sobre las últimas noticias resultará en respuestas basadas en información desactualizada o irrelevante. Problemas de Utilización de Datos: Cómo los modelos de lenguaje utilizan su base de conocimientos también puede contribuir a las alucinaciones. Los modelos de lenguaje tienden a depender de patrones de co-ocurrencia de palabras para generar respuestas. Esta dependencia podría llevar a conclusiones incorrectas, como identificar a Sídney como la capital de Australia debido a su asociación más frecuente en comparación con la respuesta correcta, Canberra. Causas Relacionadas con el Entrenamiento Sesgo de Exposición: Este problema surge de una discrepancia entre cómo se entrena al modelo y el uso real del modelo. Durante el entrenamiento, el modelo aprende a predecir el siguiente token confiando en la secuencia correcta anterior de los datos de entrenamiento, un método conocido como “enseñanza forzada”. Sin embargo, durante la inferencia, el modelo depende de sus propias predicciones. Esto puede llevar a una acumulación de errores o a un efecto bola de nieve. Limitaciones Arquitectónicas: Los modelos de lenguaje causal generan texto prediciendo el siguiente token basándose únicamente en los tokens precedentes. Este diseño limita la capacidad del modelo para captar algunas sutilezas contextuales, lo que se cree que contribuye a la generación de contenido alucinado. Desalineación de Creencias: Los LLM generalmente pasan por un proceso de entrenamiento en dos etapas. Inicialmente, son preentrenados para adquirir un conocimiento amplio. Posteriormente, son ajustados (fine-tuned) para alinearse mejor con instrucciones o indicaciones específicas de los usuarios, a menudo utilizando técnicas como el aprendizaje por refuerzo. Este ajuste podría llevar inadvertidamente al modelo a priorizar las preferencias del usuario sobre la exactitud. Causas Relacionadas con la Inferencia Aleatoriedad en el Muestreo: Las estrategias de descodificación de los modelos de lenguaje a menudo incorporan cierta aleatoriedad para favorecer la creatividad y la calidad de la respuesta, por ejemplo, ajustando el parámetro de temperatura para favorecer tokens menos probables. Este enfoque puede aumentar inadvertidamente la probabilidad de generar contenido sin sentido. Cuello de Botella de Softmax: La última capa de un LLM estima la probabilidad del siguiente token. La mayoría de los LLM utilizan una capa softmax, que se sabe que tiene una limitación llamada el cuello de botella de softmax. Esta limitación impide la capacidad del modelo para distinguir eficazmente entre resultados altamente probables. Atención Diluida: Con secuencias de entrada más largas, la eficiencia del mecanismo de atención del modelo puede disminuir, lo que lleva a una atención debilitada en detalles y contexto esenciales. Esta dilución de la atención compromete la capacidad del modelo para mantenerse coherente y preciso en textos extendidos. ¿Cómo podemos detectar las alucinaciones? Detectar alucinaciones presenta desafíos significativos que las métricas estándar utilizadas en el Procesamiento de Lenguaje Natural (NLP) como BLEU, ROUGE, o BERTScore no solucionan. No solo estas métricas no predicen con precisión las alucinaciones, sino que también dependen de la respuesta de referencia, lo cual es impracticable para aplicaciones generalizadas.\nUn enfoque para la detección de alucinaciones es emplear métricas probabilísticas que calculan la probabilidad de alucinaciones basadas en probabilidades de tokens o medidas de entropía, por ejemplo, estimando la perplejidad. Sin embargo, estas métricas requieren acceso a las probabilidades de los tokens, que pueden no estar disponibles, lo que limita su adopción.\nUna estrategia más efectiva ha sido el desarrollo de métricas basadas en modelos. Este enfoque implica utilizar otro modelo de lenguaje, por ejemplo, un LLM de última generación, para evaluar la probabilidad de contenido alucinatorio. Estas métricas se pueden clasificar aún más en métricas a nivel de token y a nivel de frase.\nLas métricas a nivel de token intentan predecir para cada token si fue alucinado o no. Para implementar esta estrategia, los investigadores generalmente generan un conjunto de datos alucinados sintéticamente aplicando perturbaciones a conjuntos de datos de referencia libres de alucinaciones. Luego se entrena un clasificador de lenguaje binario, como uno basado en BERT o XLNet, para discernir entre tokens alucinados y no alucinados. Proyectos que utilizan este enfoque son HADES y el pipeline de detección de alucinaciones fairseq-detect-hallucination.\nPor otro lado, las métricas a nivel de frase evalúan la oración completa para determinar la presencia de alucinaciones. Un método destacado es SelfCheckGPT, una técnica basada en el muestreo que se fundamenta en la suposición de que las alucinaciones son más probables de ocurrir cuando el modelo está incierto. Este método evalúa la autoconsistencia comparando varias respuestas muestreadas, utilizando métricas estándar como coincidencia de N-gramas o BERTScore, para estimar la probabilidad de contenido alucinatorio. Una fortaleza de este método es que no requiere ningún dato externo, lo que facilita su escalabilidad.\nEstrategias para Mitigar las Alucinaciones Las estrategias de mitigación de alucinaciones tienen como objetivo reducir la probabilidad de que un LLM genere contenido alucinatorio. A continuación, exploramos varias técnicas, ordenadas desde ajustes sencillos hasta intervenciones más complejas.\n1. Ajustar los Parámetros de Inferencia Uno de los métodos más sencillos para reducir las alucinaciones es ajustar la configuración operativa del LLM. Por ejemplo, afinar parámetros como la temperatura de generación y el muestreo top-k puede regular la aleatoriedad de la respuesta del LLM. Valores más bajos de temperatura o top-k pueden producir respuestas más cautelosas y conservadoras, minimizando potencialmente las salidas alucinatorias.\n2. Ingeniería de Prompts La ingeniería de indicaciones efectiva es crucial para dirigir los LLM hacia respuestas precisas. Los siguientes consejos generales son esenciales para optimizar los prompts:\nClaridad y Precisión: Asegurar que el prompt sea claro sobre el tipo exacto de información requerida.\nReconocimiento de Incertidumbre: Solicitar explícitamente que el modelo evite inventar información y exprese incertidumbre cuando sea aplicable, por ejemplo diciendo “No lo sé”.\nÉnfasis en la Información Clave: Repetir detalles importantes dentro del prompt puede ayudar a subrayar la importancia de estos elementos. Esto ayuda a asegurar que reciban la debida atención en la respuesta del modelo.\nColocación Estratégica de la Información: Posicionar la información más crucial hacia el final del prompt puede mejorar la atención del modelo en estos detalles clave.\nEspecificación del Formato de Salida: Dirigir al modelo para que estructure su respuesta en un formato específico puede ayudar a obtener información que está organizada de manera que mejor se adapte a las necesidades del usuario.\nAdemás, técnicas más avanzadas de ingeniería de prompts ofrecen capas adicionales de sofisticación:\nIndicaciones Múltiples: involucra proporcionar al modelo múltiples ejemplos del resultado deseado. Este enfoque ayuda al modelo a comprender mejor el contexto y alinear sus respuestas con el formato de respuesta esperado.\nCadena de Pensamiento: indica al modelo a mostrar su proceso de razonamiento paso a paso, ofreciendo a los usuarios percepciones sobre la progresión lógica que lleva a la respuesta.\nAutoconsistencia: implica generar varias respuestas y seleccionar la que demuestre el mayor grado de consistencia interna, mejorando así la fiabilidad.\nCadena de Verificación: es una técnica donde el modelo se utiliza para verificar sus respuestas o hacer anotaciones, mejorando aún más la precisión y la confiabilidad de la información proporcionada.\n3. Técnicas Basadas en Recuperación Estas técnicas mejoran las respuestas del modelo incorporando fuentes de conocimiento externas y contexto a la indicación. Probablemente, la técnica más conocida es Generación Aumentada por Recuperación (RAG, por sus siglas en inglés): Esta técnica codifica documentos externos utilizando sentence embeddings, generalmente mediante alguna red neuronal basada en transformadores como BERT. Las embeddings resultantes se almacenan luego en una base de datos vectorial, que luego se puede consultar de manera eficiente usando el prompt original (o alguna variación). Los datos recuperados se utilizan entonces para enriquecer el prompt. Una implementación popular es FAISS. El principal inconveniente de estos enfoques basados en recuperación es que requieren mantener una base de datos grande y curada.\n4. Ajuste Fino de LLM La calidad de la salida de un modelo depende directamente de sus datos de entrenamiento. Ajustar un LLM con datos actualizados o más relevantes puede abordar debilidades o lagunas específicas en su base de conocimiento. Aunque volver a entrenar desde cero es a menudo impracticable, el ajuste fino ofrece una alternativa rentable para mejorar la precisión del modelo en aplicaciones específicas. Técnicas como LoRA ayudan a reducir el número de parámetros entrenables lo que acelera el ajuste fino haciéndolo una opción viable.\nReflexión Final Hay una línea fina entre la creatividad y las alucinaciones. Por un lado, un modelo que se inclina demasiado hacia la precaución puede evitar las alucinaciones pero a costa de producir respuestas que son poco interesantes o demasiado simplistas. Por otro lado, un modelo excesivamente creativo producirá salidas que se asemejan a una entrevista con Salvador Dalí.\nEncontrar el equilibrio correcto es clave para aprovechar el potencial completo de los LLM mientras se minimizan los riesgos asociados con las alucinaciones. En última instancia, entender y mitigar las alucinaciones no es solo sobre prevenir errores; se trata de construir confianza entre los humanos y la inteligencia artificial, asegurando que estas herramientas mejoren nuestros procesos de toma de decisiones, creatividad y acceso a la información.\nBibliografía Huang et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. (2023) Ji et al. Survey of Hallucination in Natural Language Generation. (2022) Luo et al. Hallucination Detection and Hallucination Mitigation: An Investigation. (2024) Manakul et al. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (2023) Xu et al. Hallucination is Inevitable: An Innate Limitation of Large Language Models. (2024) Zhou et al. Detecting Hallucinated Content in Conditional Neural Sequence Generation. (2020) ","wordCount":"1978","inLanguage":"es","datePublished":"2024-03-24T00:00:00Z","dateModified":"2024-03-24T00:00:00Z","author":{"@type":"Person","name":"Olivier MF Martin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/"},"publisher":{"@type":"Organization","name":"Olivier MF Martin","logo":{"@type":"ImageObject","url":"https://omfmartin.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://omfmartin.com/es/ accesskey=h title="Olivier MF Martin (Alt + H)">Olivier MF Martin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://omfmartin.com/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://omfmartin.com/es/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://omfmartin.com/es/sobre-mi title="Sobre Mí"><span>Sobre Mí</span></a></li><li><a href=https://omfmartin.com/es/buscador title="Buscador (Alt + /)" accesskey=/><span>Buscador</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Entender y Mitigar las Alucinaciones de los LLMs</h1><div class=post-meta><span title='2024-03-24 00:00:00 +0000 UTC'>24/03/2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Olivier MF Martin&nbsp;|&nbsp;Traducciones:<ul class=i18n_list><li><a href=https://omfmartin.com/posts/2024-03-24-llm-hallucination/>English</a></li></ul></div></header><div class=post-content><p><strong>Los Modelos de Lenguaje de Gran Tamaño (LLMs) inventan cosas</strong>. Ocasionalmente, estos modelos generarán contenido que, aunque parezca totalmente plausible, es fundamentalmente incorrecto. Este fenómeno se conoce como &ldquo;alucinaciones&rdquo;.</p><p><strong>Las alucinaciones pueden tener consecuencias desastrosas</strong>. Pueden difundir información errónea, potencialmente ofender a usuarios, e incluso comprometer información sensible. Este problema es un gran inconveniente de los LLMs y es la razón por la que muchos son cautelosos a la hora de emplearlos en entornos del mundo real.</p><p>En este artículo, discutiremos qué son las alucinaciones, explicaremos por qué ocurren y exploraremos estrategias para detectarlas y mitigarlas.</p><p><img loading=lazy src=/posts/2024-03-24-llm-hallucination/hallucinating_llm.png alt="Una pintura surrealista y psicodélica que retrata un modelo lingüístico embarcándose en un viaje alucinatorio y alterador de la mente."></p><h2 id=qué-son-las-alucinaciones>¿Qué son las Alucinaciones?<a hidden class=anchor aria-hidden=true href=#qué-son-las-alucinaciones>#</a></h2><p>Las alucinaciones pueden definirse de manera general como respuestas que son <strong>factualmente incorrectas, infieles a la información proporcionada, o sin sentido</strong>. La mayoría de las veces, el modelo parecerá bastante seguro y su respuesta puede incluso parecer totalmente plausible. Esto hace que detectar alucinaciones sea una tarea difícil. Generalmente, las alucinaciones se dividen en dos categorías: intrínsecas y extrínsecas.</p><p><strong>Las alucinaciones intrínsecas</strong> ocurren cuando la respuesta del modelo contradice directamente la información que se le dio. Por ejemplo:</p><blockquote><blockquote><blockquote><p><strong>Prompt</strong>: La capital de Francia es París. ¿Cuál es la capital de Francia?
<strong>Respuesta</strong>: La capital de Francia es Lyon.</p></blockquote></blockquote></blockquote><p><strong>Las alucinaciones extrínsecas</strong> se producen cuando el modelo realiza afirmaciones que no pueden ser verificadas ni contradichas con la información proporcionada. Por ejemplo:</p><blockquote><blockquote><blockquote><p><strong>Prompt</strong>: Describe el impacto económico de las fuentes de energía renovable.
<strong>Respuesta</strong>: Según un informe de 2045, las fuentes de energía renovable han llevado a la creación de 5 millones de nuevos empleos en todo el mundo.</p></blockquote></blockquote></blockquote><h2 id=por-qué-los-llms-alucinan>¿Por qué los LLMs Alucinan?<a hidden class=anchor aria-hidden=true href=#por-qué-los-llms-alucinan>#</a></h2><p>Se considera que las alucinaciones en los modelos de lenguaje son inevitables. Pueden ocurrir debido a una variedad de factores, los cuales se pueden clasificar de manera general en problemas relacionados con los datos, el proceso de entrenamiento y la inferencia.</p><h3 id=causas-relacionadas-con-los-datos>Causas Relacionadas con los Datos<a hidden class=anchor aria-hidden=true href=#causas-relacionadas-con-los-datos>#</a></h3><ul><li><strong>Inexactitudes en los Datos de Entrenamiento</strong>: Los datos de entrenamiento son la base de los LLMs. Sin embargo, los datos no siempre son de la mejor calidad y pueden ser inexactos o sesgados. Dada la vasta cantidad de datos procesados, asegurar su precisión y relevancia es un desafío. Esto implica que parte de la información aprendida por el modelo lo llevará a generar respuestas engañosas o incorrectas.</li><li><strong>Consultas Fuera de Distribución</strong>: Los LLMs se entrenan en conjuntos de datos que cubren ciertos temas, períodos de tiempo y tipos de información. Cuando se enfrentan a preguntas o tareas que se encuentran fuera de estos límites, un modelo de lenguaje puede generar respuestas incorrectas. Por ejemplo, preguntar sobre las últimas noticias resultará en respuestas basadas en información desactualizada o irrelevante.</li><li><strong>Problemas de Utilización de Datos</strong>: Cómo los modelos de lenguaje utilizan su base de conocimientos también puede contribuir a las alucinaciones. Los modelos de lenguaje tienden a depender de patrones de co-ocurrencia de palabras para generar respuestas. Esta dependencia podría llevar a conclusiones incorrectas, como identificar a Sídney como la capital de Australia debido a su asociación más frecuente en comparación con la respuesta correcta, Canberra.</li></ul><h3 id=causas-relacionadas-con-el-entrenamiento>Causas Relacionadas con el Entrenamiento<a hidden class=anchor aria-hidden=true href=#causas-relacionadas-con-el-entrenamiento>#</a></h3><ul><li><strong>Sesgo de Exposición</strong>: Este problema surge de una discrepancia entre cómo se entrena al modelo y el uso real del modelo. Durante el entrenamiento, el modelo aprende a predecir el siguiente <em>token</em> confiando en la secuencia correcta anterior de los datos de entrenamiento, un método conocido como &ldquo;enseñanza forzada&rdquo;. Sin embargo, durante la inferencia, el modelo depende de sus propias predicciones. Esto puede llevar a una acumulación de errores o a un efecto bola de nieve.</li><li><strong>Limitaciones Arquitectónicas</strong>: Los modelos de lenguaje causal generan texto prediciendo el siguiente token basándose únicamente en los <em>tokens</em> precedentes. Este diseño limita la capacidad del modelo para captar algunas sutilezas contextuales, lo que se cree que contribuye a la generación de contenido alucinado.</li><li><strong>Desalineación de Creencias</strong>: Los LLM generalmente pasan por un proceso de entrenamiento en dos etapas. Inicialmente, son preentrenados para adquirir un conocimiento amplio. Posteriormente, son ajustados (<em>fine-tuned</em>) para alinearse mejor con instrucciones o indicaciones específicas de los usuarios, a menudo utilizando técnicas como el <a href=https://es.wikipedia.org/wiki/Aprendizaje_por_refuerzo>aprendizaje por refuerzo</a>. Este ajuste podría llevar inadvertidamente al modelo a priorizar las preferencias del usuario sobre la exactitud.</li></ul><h3 id=causas-relacionadas-con-la-inferencia>Causas Relacionadas con la Inferencia<a hidden class=anchor aria-hidden=true href=#causas-relacionadas-con-la-inferencia>#</a></h3><ul><li><strong>Aleatoriedad en el Muestreo</strong>: Las estrategias de descodificación de los modelos de lenguaje a menudo incorporan cierta aleatoriedad para favorecer la creatividad y la calidad de la respuesta, por ejemplo, ajustando el parámetro de temperatura para favorecer <em>tokens</em> menos probables. Este enfoque puede aumentar inadvertidamente la probabilidad de generar contenido sin sentido.</li><li><strong>Cuello de Botella de Softmax</strong>: La última capa de un LLM estima la probabilidad del siguiente <em>token</em>. La mayoría de los LLM utilizan una capa softmax, que se sabe que tiene una limitación llamada el cuello de botella de softmax. Esta limitación impide la capacidad del modelo para distinguir eficazmente entre resultados altamente probables.</li><li><strong>Atención Diluida</strong>: Con secuencias de entrada más largas, la eficiencia del mecanismo de atención del modelo puede disminuir, lo que lleva a una atención debilitada en detalles y contexto esenciales. Esta dilución de la atención compromete la capacidad del modelo para mantenerse coherente y preciso en textos extendidos.</li></ul><h2 id=cómo-podemos-detectar-las-alucinaciones>¿Cómo podemos detectar las alucinaciones?<a hidden class=anchor aria-hidden=true href=#cómo-podemos-detectar-las-alucinaciones>#</a></h2><p>Detectar alucinaciones presenta desafíos significativos que las métricas estándar utilizadas en el Procesamiento de Lenguaje Natural (NLP) como <a href=https://es.wikipedia.org/wiki/BLEU>BLEU</a>, <a href=https://es.wikipedia.org/wiki/ROUGE_(m%C3%A9trica)>ROUGE</a>, o <a href=https://en.wikipedia.org/wiki/Sentence_embedding>BERTScore</a> no solucionan. No solo estas métricas no predicen con precisión las alucinaciones, sino que también dependen de la respuesta de referencia, lo cual es impracticable para aplicaciones generalizadas.</p><p>Un enfoque para la detección de alucinaciones es emplear <strong>métricas probabilísticas</strong> que calculan la probabilidad de alucinaciones basadas en probabilidades de <em>tokens</em> o medidas de entropía, por ejemplo, estimando la <a href=https://es.wikipedia.org/wiki/Perplejidad>perplejidad</a>. Sin embargo, estas métricas requieren acceso a las probabilidades de los <em>tokens</em>, que pueden no estar disponibles, lo que limita su adopción.</p><p>Una estrategia más efectiva ha sido el desarrollo de <strong>métricas basadas en modelos</strong>. Este enfoque implica utilizar otro modelo de lenguaje, por ejemplo, un LLM de última generación, para evaluar la probabilidad de contenido alucinatorio. Estas métricas se pueden clasificar aún más en métricas a nivel de <em>token</em> y a nivel de frase.</p><p>Las <strong>métricas a nivel de <em>token</em></strong> intentan predecir para cada <em>token</em> si fue alucinado o no. Para implementar esta estrategia, los investigadores generalmente generan un conjunto de datos alucinados sintéticamente aplicando perturbaciones a conjuntos de datos de referencia libres de alucinaciones. Luego se entrena un clasificador de lenguaje binario, como uno basado en <a href=https://es.wikipedia.org/wiki/BERT_(modelo_de_lenguaje)>BERT</a> o <a href=https://doi.org/10.48550/arXiv.1906.08237>XLNet</a>, para discernir entre <em>tokens</em> alucinados y no alucinados. Proyectos que utilizan este enfoque son <a href=https://github.com/microsoft/HaDes>HADES</a> y el pipeline de detección de alucinaciones <a href=https://github.com/violet-zct/fairseq-detect-hallucination>fairseq-detect-hallucination</a>.</p><p>Por otro lado, las <strong>métricas a nivel de frase</strong> evalúan la oración completa para determinar la presencia de alucinaciones. Un método destacado es <a href=https://github.com/potsawee/selfcheckgpt>SelfCheckGPT</a>, una técnica basada en el muestreo que se fundamenta en la suposición de que las alucinaciones son más probables de ocurrir cuando el modelo está incierto. Este método evalúa la autoconsistencia comparando varias respuestas muestreadas, utilizando métricas estándar como coincidencia de N-gramas o BERTScore, para estimar la probabilidad de contenido alucinatorio. Una fortaleza de este método es que no requiere ningún dato externo, lo que facilita su escalabilidad.</p><h2 id=estrategias-para-mitigar-las-alucinaciones>Estrategias para Mitigar las Alucinaciones<a hidden class=anchor aria-hidden=true href=#estrategias-para-mitigar-las-alucinaciones>#</a></h2><p>Las estrategias de mitigación de alucinaciones tienen como objetivo reducir la probabilidad de que un LLM genere contenido alucinatorio. A continuación, exploramos varias técnicas, ordenadas desde ajustes sencillos hasta intervenciones más complejas.</p><p><img loading=lazy src=/posts/2024-03-24-llm-hallucination/llm_waking_up.png alt="Visualiza un momento capturando las secuelas de un viaje alucinatorio para un modelo lingüístico."></p><h3 id=1-ajustar-los-parámetros-de-inferencia>1. Ajustar los Parámetros de Inferencia<a hidden class=anchor aria-hidden=true href=#1-ajustar-los-parámetros-de-inferencia>#</a></h3><p>Uno de los métodos más sencillos para reducir las alucinaciones es ajustar la configuración operativa del LLM. Por ejemplo, afinar parámetros como la temperatura de generación y el muestreo <em>top-k</em> puede regular la aleatoriedad de la respuesta del LLM. Valores más bajos de temperatura o <em>top-k</em> pueden producir respuestas más cautelosas y conservadoras, minimizando potencialmente las salidas alucinatorias.</p><h3 id=2-ingeniería-de-prompts>2. Ingeniería de Prompts<a hidden class=anchor aria-hidden=true href=#2-ingeniería-de-prompts>#</a></h3><p>La ingeniería de indicaciones efectiva es crucial para dirigir los LLM hacia respuestas precisas. Los siguientes consejos generales son esenciales para optimizar los <em>prompts</em>:</p><ul><li><p><strong>Claridad y Precisión</strong>: Asegurar que el <em>prompt</em> sea claro sobre el tipo exacto de información requerida.</p></li><li><p><strong>Reconocimiento de Incertidumbre</strong>: Solicitar explícitamente que el modelo evite inventar información y exprese incertidumbre cuando sea aplicable, por ejemplo diciendo &ldquo;No lo sé&rdquo;.</p></li><li><p><strong>Énfasis en la Información Clave</strong>: Repetir detalles importantes dentro del <em>prompt</em> puede ayudar a subrayar la importancia de estos elementos. Esto ayuda a asegurar que reciban la debida atención en la respuesta del modelo.</p></li><li><p><strong>Colocación Estratégica de la Información</strong>: Posicionar la información más crucial hacia el final del <em>prompt</em> puede mejorar la atención del modelo en estos detalles clave.</p></li><li><p><strong>Especificación del Formato de Salida</strong>: Dirigir al modelo para que estructure su respuesta en un formato específico puede ayudar a obtener información que está organizada de manera que mejor se adapte a las necesidades del usuario.</p></li></ul><p>Además, técnicas más avanzadas de <a href=https://www.promptingguide.ai>ingeniería de <em>prompts</em></a> ofrecen capas adicionales de sofisticación:</p><ul><li><p><strong>Indicaciones Múltiples</strong>: involucra proporcionar al modelo múltiples ejemplos del resultado deseado. Este enfoque ayuda al modelo a comprender mejor el contexto y alinear sus respuestas con el formato de respuesta esperado.</p></li><li><p><strong>Cadena de Pensamiento</strong>: indica al modelo a mostrar su proceso de razonamiento paso a paso, ofreciendo a los usuarios percepciones sobre la progresión lógica que lleva a la respuesta.</p></li><li><p><strong>Autoconsistencia</strong>: implica generar varias respuestas y seleccionar la que demuestre el mayor grado de consistencia interna, mejorando así la fiabilidad.</p></li><li><p><strong>Cadena de Verificación</strong>: es una técnica donde el modelo se utiliza para verificar sus respuestas o hacer anotaciones, mejorando aún más la precisión y la confiabilidad de la información proporcionada.</p></li></ul><h3 id=3-técnicas-basadas-en-recuperación>3. Técnicas Basadas en Recuperación<a hidden class=anchor aria-hidden=true href=#3-técnicas-basadas-en-recuperación>#</a></h3><p>Estas técnicas mejoran las respuestas del modelo incorporando fuentes de conocimiento externas y contexto a la indicación. Probablemente, la técnica más conocida es <strong>Generación Aumentada por Recuperación (RAG, por sus siglas en inglés)</strong>: Esta técnica codifica documentos externos utilizando <a href=https://en.wikipedia.org/wiki/Sentence_embedding><em>sentence embeddings</em></a>, generalmente mediante alguna red neuronal basada en transformadores como <a href=https://es.wikipedia.org/wiki/BERT_(modelo_de_lenguaje)>BERT</a>. Las <em>embeddings</em> resultantes se almacenan luego en una <a href=https://en.wikipedia.org/wiki/Vector_database>base de datos vectorial</a>, que luego se puede consultar de manera eficiente usando el <em>prompt</em> original (o alguna variación). Los datos recuperados se utilizan entonces para enriquecer el <em>prompt</em>. Una implementación popular es <a href=https://github.com/facebookresearch/faiss>FAISS</a>. El principal inconveniente de estos enfoques basados en recuperación es que requieren mantener una base de datos grande y curada.</p><h3 id=4-ajuste-fino-de-llm>4. Ajuste Fino de LLM<a hidden class=anchor aria-hidden=true href=#4-ajuste-fino-de-llm>#</a></h3><p>La calidad de la salida de un modelo depende directamente de sus datos de entrenamiento. Ajustar un LLM con datos actualizados o más relevantes puede abordar debilidades o lagunas específicas en su base de conocimiento. Aunque volver a entrenar desde cero es a menudo impracticable, el ajuste fino ofrece una alternativa rentable para mejorar la precisión del modelo en aplicaciones específicas. Técnicas como <a href=https://doi.org/10.48550/arXiv.2106.09685>LoRA</a> ayudan a reducir el número de parámetros entrenables lo que acelera el ajuste fino haciéndolo una opción viable.</p><h2 id=reflexión-final>Reflexión Final<a hidden class=anchor aria-hidden=true href=#reflexión-final>#</a></h2><p>Hay una línea fina entre la creatividad y las alucinaciones. Por un lado, un modelo que se inclina demasiado hacia la precaución puede evitar las alucinaciones pero a costa de producir respuestas que son poco interesantes o demasiado simplistas. Por otro lado, un modelo excesivamente creativo producirá salidas que se asemejan a <a href="https://www.youtube.com/watch?v=A3FAy0teMNo">una entrevista con Salvador Dalí</a>.</p><p>Encontrar el equilibrio correcto es clave para aprovechar el potencial completo de los LLM mientras se minimizan los riesgos asociados con las alucinaciones. En última instancia, entender y mitigar las alucinaciones no es solo sobre prevenir errores; se trata de construir confianza entre los humanos y la inteligencia artificial, asegurando que estas herramientas mejoren nuestros procesos de toma de decisiones, creatividad y acceso a la información.</p><h2 id=bibliografía>Bibliografía<a hidden class=anchor aria-hidden=true href=#bibliografía>#</a></h2><ul><li><a href=https://doi.org/10.48550/arXiv.2311.05232>Huang et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. (2023)</a></li><li><a href=https://doi.org/10.1145/3571730>Ji et al. Survey of Hallucination in Natural Language Generation. (2022)</a></li><li><a href=https://doi.org/10.48550/arXiv.2401.08358>Luo et al. Hallucination Detection and Hallucination Mitigation: An Investigation. (2024)</a></li><li><a href=https://doi.org/10.48550/arXiv.2303.08896>Manakul et al. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (2023)</a></li><li><a href=https://doi.org/10.48550/arXiv.2401.11817>Xu et al. Hallucination is Inevitable: An Innate Limitation of Large Language Models. (2024)</a></li><li><a href=https://doi.org/10.48550/arXiv.2011.02593>Zhou et al. Detecting Hallucinated Content in Conditional Neural Sequence Generation. (2020)</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://omfmartin.com/es/posts/2024-02-18-explainable-ai/><span class=title>Siguiente »</span><br><span>Una Guía Rápida sobre la IA Explicable</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://omfmartin.com/es/>Olivier MF Martin</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copiar";function s(){t.innerHTML="¡copiado!",setTimeout(()=>{t.innerHTML="copiar"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>