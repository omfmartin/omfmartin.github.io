<!doctype html><html lang=es dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Una Guía Rápida sobre la IA Explicable | Olivier MF Martin</title>
<meta name=keywords content><meta name=description content="Abriendo la caja negra de la IA para entender cómo razonan los algoritmos y fomentar la confianza."><meta name=author content="Olivier MF Martin"><link rel=canonical href=https://omfmartin.com/es/posts/2024-02-18-explainable-ai/><link crossorigin=anonymous href=/assets/css/stylesheet.c7ada1ec5c18a60093054264541f69ef87055a2648305df1f1d6dd520e8e1f5e.css integrity="sha256-x62h7FwYpgCTBUJkVB9p74cFWiZIMF3x8dbdUg6OH14=" rel="preload stylesheet" as=style><link rel=icon href=https://omfmartin.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://omfmartin.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://omfmartin.com/favicon-32x32.png><link rel=apple-touch-icon href=https://omfmartin.com/apple-touch-icon.png><link rel=mask-icon href=https://omfmartin.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://omfmartin.com/posts/2024-02-18-explainable-ai/><link rel=alternate hreflang=es href=https://omfmartin.com/es/posts/2024-02-18-explainable-ai/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=/assets/css/extended/style.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-PKR8VY4E8L"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PKR8VY4E8L",{anonymize_ip:!1})}</script><meta property="og:title" content="Una Guía Rápida sobre la IA Explicable"><meta property="og:description" content="Abriendo la caja negra de la IA para entender cómo razonan los algoritmos y fomentar la confianza."><meta property="og:type" content="article"><meta property="og:url" content="https://omfmartin.com/es/posts/2024-02-18-explainable-ai/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Una Guía Rápida sobre la IA Explicable"><meta name=twitter:description content="Abriendo la caja negra de la IA para entender cómo razonan los algoritmos y fomentar la confianza."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://omfmartin.com/es/posts/"},{"@type":"ListItem","position":2,"name":"Una Guía Rápida sobre la IA Explicable","item":"https://omfmartin.com/es/posts/2024-02-18-explainable-ai/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Una Guía Rápida sobre la IA Explicable","name":"Una Guía Rápida sobre la IA Explicable","description":"Abriendo la caja negra de la IA para entender cómo razonan los algoritmos y fomentar la confianza.","keywords":[],"articleBody":"¿Qué es la IA Explicable? Cada año, la inteligencia artificial (IA) y los modelos de aprendizaje automático se vuelven más complejos, gracias a ordenadores más rápidos, algoritmos más ágiles y más datos. Ciertos modelos ahora se componen de cientos de miles de millones de parámetros. Esta complejidad les hace difíciles, si no imposibles, de entender.\nEl aumento en el número de parámetros del modelo entre 1954 y 2024. Datos de Jaime Sevilla. Aquí es donde entra en escena la IA explicable (XAI, por sus siglas en inglés). La IA explicable busca abrir cajas negras, proporcionando insights sobre cómo y por qué los algoritmos hacen sus predicciones. Al permitirnos explorar cómo las entradas de datos influyen en las predicciones del modelo, la IA explicable hace que la IA sea más transparente, fomenta la confianza y permite que tanto los desarrolladores como los usuarios finales comprendan el razonamiento del modelo.\nEste artículo ofrece una breve introducción a la IA explicable, destacando su importancia y las metodologías involucradas. Para aquellos que busquen un análisis más profundo del tema, la sección de recursos al final de este artículo ofrece una selección de libros y herramientas de software dedicadas a IA explicable.\nCasos de Uso para Abrir la Caja Negra Depuración y Mejora de Modelos Los modelos predictivos cometen errores. Comprender las causas raíz que conducen a estas inexactitudes es esencial para mejorar el rendimiento del modelo. Al ayudarnos a entender el razonamiento detrás de las predicciones, las técnicas de IA explicable nos ayudan a caracterizar las situaciones donde el modelo está cometiendo errores. Esto hace de la IA explicable una herramienta valiosa para la depuración de modelos y contribuye al refinamiento del modelo.\nConsideremos un escenario que involucra una tarea de clasificación de animales. Las técnicas de IA explicable, como los mapas de saliencia, pueden resaltar las partes de una imagen que el clasificador considera significativas para hacer una predicción. Este análisis podría revelar que, en el caso de una imagen de un camello, el modelo erróneamente se concentra en la arena en lugar del camello en sí. Esto ayudaría a explicar por qué el modelo es incapaz de clasificar con precisión imágenes de camellos en otros entornos.\nDetección y Mitigación de Sesgos Los modelos predictivos pueden estar sesgados, lo que conduce a desigualdades sociales y perjudica a los grupos minoritarios. Los sesgos algorítmicos suelen provenir del uso de datos sesgados o incompletos, o de las suposiciones integradas en los modelos.\nUn ejemplo notable de tal sesgo se observó en un algoritmo comercial utilizado por el sistema de salud de EE.UU.. Específicamente, para idénticas puntuaciones de riesgo predichas, se encontró que los pacientes negros estaban más enfermos que sus homólogos blancos. Este sesgo provino del modelo que suponía que mayores costos de atención médica significaban mayores necesidades de atención médica. Dado que se gastaba menos dinero en la población negra, el algoritmo concluyó erróneamente que corrían menos riesgo.\nInstancias similares de sesgo algorítmico han sido documentadas en el sistema judicial y procesos de solicitud de empleo. Estos ejemplos destacan la necesidad de responsabilidad en los sistemas de IA, especialmente cuando se despliegan en áreas con impactos sociales significativos.\nLas técnicas de IA explicable pueden ayudar a identificar estos sesgos contextualizando por qué los modelos hacen ciertas predicciones. Por ejemplo, en el ejemplo de atención médica mencionado anteriormente, la IA explicable podría demostrar cómo ser negro bajaba la puntuación de riesgo. Por lo tanto, la IA explicable es instrumental no solo en la interpretación de predicciones de modelos sino también en impulsar el desarrollo de sistemas de IA más equitativos.\nEscenarios de Alto Riesgo Predicciones erróneas o sesgadas pueden impactar profundamente las vidas y el bienestar de los individuos, especialmente cuando se utilizan en escenarios de alto riesgo. Por ejemplo, al usar algoritmos predictivos para sistemas de puntuación social, categorización individual, tecnologías de reconocimiento facial, herramientas de aplicación de la ley, procesos de selección de empleo y la provisión de servicios esenciales como la atención sanitaria.\nEn la práctica, la explicación puede ser tan importante como la predicción. Por esta razón, en sectores de alto riesgo como la atención sanitaria, finanzas y justicia penal, hay una marcada preferencia por emplear modelos inherentemente interpretables. Por ejemplo, la regresión logística, con su interpretabilidad y simplicidad, a menudo se prefiere sobre redes neuronales más complejas dentro de los entornos de atención sanitaria.\nCumplimiento Regulatorio A medida que el despliegue de la IA se vuelve más generalizado, las leyes y regulaciones están comenzando a requerir que las decisiones de IA sean transparentes y explicables. Este cambio tiene como objetivo garantizar que estos algoritmos operen dentro de límites éticos.\nPor ejemplo, el Reglamento General de Protección de Datos (GDPR) de la Unión Europea exige una transparencia significativa sobre la lógica involucrada en las decisiones automatizadas, particularmente en decisiones que afectan significativamente a los individuos, como aquellas relacionadas con el empleo, la solvencia y asuntos legales. Esta regulación ha sido interpretada por algunos como un derecho a la explicación. Basándose en esta base, el más reciente AI Act ha introducido disposiciones específicas para algoritmos de “alto riesgo”, con requisitos relacionados con la “transparencia y provisión de información a los usuarios”.\nSin embargo, estas regulaciones enfrentan desafíos en la aplicación e interpretación. La complejidad inherente de los algoritmos de IA puede hacer que sea desafiante proporcionar explicaciones que sean técnicamente precisas y fácilmente comprensibles para el público general. A pesar de estos desafíos, la presión por tales regulaciones destaca la creciente preocupación por la transparencia en las aplicaciones de IA y subraya la necesidad crítica de la IA explicable.\nDescubrimiento de Conocimiento El uso de la IA explicable para generar nuevo conocimiento es probablemente la aplicación más pasada por alto. Esta capacidad es particularmente interesante dado que los modelos de aprendizaje automático son más aptos para identificar patrones y tendencias complejas en los datos en comparación con pruebas estadísticas tradicionales como las pruebas t o chi-cuadrado. Este enfoque fue sugerido para el descubrimiento de biomarcadores.\nA pesar de su potencial, emplear la IA explicable para la generación de conocimiento no está exento de desafíos. Algunos de los problemas más significativos incluyen:\nAmbigüedad del Umbral de Significancia: A diferencia de los métodos estadísticos tradicionales, que tienen criterios bien definidos para la significancia estadística, la IA explicable carece de un marco estandarizado para determinar el nivel de significancia de las variables. Limitaciones de Datos Observacionales: La mayoría de los datos utilizados en el aprendizaje automático son observacionales, lo que es inherentemente menos efectivo para establecer relaciones causales. Sobreajuste: Existe el riesgo de que los modelos confundan el ruido aleatorio con patrones significativos, lo que potencialmente podría llevar a interpretaciones y conclusiones incorrectas sobre los datos. ¿Cuándo podemos prescindir de una explicación? No todas las predicciones hechas por la IA necesitan ser explicadas. Generar explicaciones introduce complejidad adicional y necesidades computacionales, que se traducen en gastos financieros e impacto ambiental. Implementar IA explicable puede ser injustificado en ciertos escenarios, tales como:\nEscenarios de Bajo Riesgo: En situaciones donde las consecuencias de los errores son mínimas, como las recomendaciones en una plataforma de streaming. Los usuarios típicamente tienen poco interés en entender el funcionamiento interno de cómo se generan estas sugerencias. Problemas Bien Entendidos: Para problemas bien establecidos como el filtrado de spam en sistemas de correo electrónico, las explicaciones para cada decisión pueden no ser esenciales porque la fiabilidad y precisión general del sistema ya son bien comprendidas. Riesgos de Manipulación: En algunos casos, proporcionar explicaciones detalladas puede ayudar involuntariamente a aquellos que buscan explotar el sistema. Por ejemplo, los expertos en SEO podrían malusar información detallada sobre algoritmos de búsqueda para manipular los rankings y socavar la integridad del sistema. Enfoques para Explicar la IA Los intrincados detalles matemáticos y cálculos de un algoritmo usualmente no revelan mucho sobre cómo toma decisiones. Esto se debe a que las representaciones aprendidas por el modelo predictivo típicamente están más allá de la capacidad de comprensión del cerebro humano. Debido a esto, los enfoques directos, como divulgar el código fuente o los cálculos matemáticos, no logran proporcionar comprensión.\nPara superar este desafío, las técnicas de IA explicable utilizan modelos inherentemente interpretables o aproximan modelos complejos de caja negra con otros más simples y comprensibles. La IA explicable es un campo activo de investigación. Actualmente, no existe una manera universal de explicar algoritmos predictivos complejos, y podría nunca haberla.\nNuestro enfoque será en categorizar las técnicas utilizadas para explicar modelos predictivos, destacando algunos de los métodos más comúnmente empleados.\nEtapa: Ante-hoc vs Post-hoc Ante-hoc (Interpretabilidad Intrínseca): Este enfoque se centra en el uso de modelos que son inherentemente interpretables debido a su estructura simple y procesos de toma de decisiones transparentes. Ejemplos incluyen modelos lineales, árboles de decisión y sistemas basados en reglas. Aunque estos modelos ofrecen claridad desde el principio, su interpretabilidad puede disminuir a medida que la complejidad o el número de parámetros aumenta. Actualmente hay investigaciones para diseñar modelos de alto rendimiento pero inherentemente interpretables; sin embargo, hasta ahora no han tenido mucho éxito. Post-hoc (Interpretabilidad Extrínseca): Los métodos post-hoc tienen como objetivo explicar decisiones tomadas por modelos complejos, cajas negras, después de haber sido entrenados. Estas explicaciones no están integradas en el modelo sino que se derivan de analizar el comportamiento del modelo. Este enfoque es particularmente útil para modelos de cajas negras, como las redes neuronales profundas. Las explicaciones post-hoc buscan ser localmente fieles, lo que significa que reflejan con precisión el razonamiento del modelo para instancias o decisiones específicas. Modelo: Específico del Modelo vs Agnóstico del Modelo Específico del Modelo: Estas técnicas solo son aplicables a tipos específicos de modelos, explotando sus arquitecturas únicas para proporcionar explicaciones. Para los modelos de aprendizaje profundo, técnicas como los Mapas de Saliencia, DeepLIFT y los Mapas de Activación de Clase (CAM) ilustran cómo diferentes características influencian las predicciones del modelo. Agnóstico del Modelo: Estas técnicas ofrecen una alternativa flexible. Los métodos agnósticos del modelo no requieren conocimiento del funcionamiento interno del modelo, haciéndolos aplicables a una amplia gama de sistemas de IA. Técnicas como SHAP (SHapley Additive exPlanations) y LIME (Explicaciones Locales Interpretables Agnósticas del Modelo) pueden usarse para explicar cualquier modelo aproximando su límite de decisión o salida de una manera interpretable. Alcance: Global vs Local Explicaciones Locales: Estas se centran en predicciones o decisiones individuales, proporcionando insights sobre por qué el modelo tomó una elección particular para una instancia específica. Herramientas como SHAP y LIME ofrecen explicaciones locales y muestran la influencia de varias características en un único resultado. Explicaciones Globales: En contraste, las explicaciones globales buscan mostrar el comportamiento del modelo a través del conjunto de datos completo o en un sentido más general. Técnicas para lograr explicaciones globales incluyen rankings de importancia de características y gráficas de dependencia parcial, que agregan los efectos de las características a través de múltiples instancias para ofrecer una comprensión más amplia de la lógica del modelo. Las Limitaciones de la IA explicable A medida que la IA continúa permeando varios aspectos de nuestras vidas, el imperativo de transparencia y comprensibilidad en los sistemas de IA nunca ha sido más pronunciado. Sin embargo, como en cualquier dominio en rápida evolución, la IA explicable no está exento de desafíos y limitaciones.\nEl tradeoff entre complejidad y rendimiento El tradeoff entre complejidad y rendimiento sugiere que mejorar el rendimiento de los modelos de IA generalmente requiere aumentar su complejidad. Esta suposición subraya la necesidad percibida de equilibrar la complejidad contra el rendimiento. Sin embargo, esta perspectiva merece un examen crítico. La hipótesis del boleto de lotería propone que la eficacia de los modelos grandes podría derivarse de una inicialización óptima de parámetros en lugar de su mero tamaño. Empíricamente, se ha demostrado que modelos más simples pueden alcanzar un rendimiento comparable al de sus contrapartes más complejas.\nDe hecho, la evidencia muestra que modelos con menos parámetros, como Mistral-7B, pueden alcanzar niveles de rendimiento comparables a los de sus contrapartes más complejas, como GPT-3.5, con una fracción de los parámetros (7 mil millones vs. 175 mil millones). Esta observación desafía la necesidad de modelos excesivamente complejos, abogando por un enfoque más medido en el diseño de modelos donde la simplicidad no excluye la efectividad.\nAlgunos investigadores argumentan que los modelos no inherentemente interpretables deberían evitarse por completo en escenarios de alto riesgo. Sin embargo, sigue sin estar claro si esto es factible en todos los contextos sin sacrificar el rendimiento.\nExplicando la Explicación Para una implementación efectiva de las técnicas de IA explicable, es crucial entender quién es el público objetivo de las explicaciones. La IA explicable sirve principalmente a profesionales técnicos, como científicos de datos e ingenieros de aprendizaje automático, que tienen un profundo entendimiento de estadística.\nSin embargo, existe una distinción significativa entre las explicaciones adecuadas para expertos en IA y aquellas para usuarios no técnicos. Los detalles técnicos pueden abrumar a los usuarios no técnicos, llevando a la confusión o malinterpretación. Por ejemplo, alguien con conocimiento limitado de estadística podría interpretar erróneamente los valores SHAP como indicativos de causalidad, en lugar de mera correlación.\nReconocer el conocimiento, objetivos, habilidades y capacidades del usuario final es primordial al usar la IA explicable. Además, la comunicación de las explicaciones de IA debe estar alineada con el nivel de comprensión de la audiencia. Esto es necesario para evitar tanto los riesgos de simplificación excesiva, que pueden llevar a malentendidos, como la ofuscación, que puede alienar y frustrar a los usuarios.\nFidelidad y los Límites de la Explicación Las explicaciones post-hoc de modelos complejos se logran aproximando localmente su comportamiento alrededor de un punto de datos específico utilizando un modelo más simple y transparente. Aunque estas aproximaciones ofrecen insights, no logran capturar completamente el comportamiento del modelo original. Además, los métodos de IA explicable a menudo carecen de métricas para evaluar la calidad de estas aproximaciones, planteando preguntas sobre la fidelidad de tales explicaciones.\nAdicionalmente, a medida que la complejidad de los modelos aumenta anualmente, nuestra capacidad para explicarlos con precisión disminuye. Esta creciente complejidad destaca una pregunta crucial: ¿Existe un límite teórico a nuestra capacidad para hacer transparentes los procesos de toma de decisiones de la IA? Y si tal límite existe, ¿cuán cerca estamos de alcanzarlo?\nConclusión La IA explicable juega un papel crítico en abordar la opacidad de los complejos modelos de aprendizaje automático, permitiendo confiar en las decisiones basadas en IA. La IA facilita identificar sesgos, depurar modelos, cumplir con regulaciones y generar nuevo conocimiento explicando el comportamiento del modelo. Existen numerosos recursos de código abierto disponibles para la implementación fácil y eficiente de IA explicable. No obstante, es importante reconocer las limitaciones y desafíos asociados con IA explicable y favorecer modelos más simples siempre que sea posible.\nRecursos Libros Análisis de Modelos Explicativos Aprendizaje Automático Interpretable Software SHAP LIME DALEX XAITK InterpretML Bibliografía Ebers. Regulating Explainable AI in the European Union. An Overview of the Current Legal Framework(s). (2021)\nFrankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. (2018)\nJiang and Senge. On Two XAI Cultures: A Case Study of Non-technical Explanations in Deployed AI System. (2021)\nGunning et al. DARPA’s explainable AI (XAI) program: A retrospective. (2021)\nLarson et al. How We Analyzed the COMPAS Recidivism Algorithm. (2016)\nLinardatos et al. Explainable AI: A Review of Machine Learning Interpretability Methods. (2020)\nLungberg and Lee. A Unified Approach to Interpreting Model Predictions. (2017)\nNg et al. The benefits and pitfalls of machine learning for biomarker discovery. (2023)\nObermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. (2019)\nRibeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. (2016)\nRudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. (2019)\nSarkar. Is explainable AI a race against model complexity? (2022)\nSevilla. Parameter counts in Machine Learning. (2021)\n","wordCount":"2633","inLanguage":"es","datePublished":"2024-02-18T00:00:00Z","dateModified":"2024-02-18T00:00:00Z","author":{"@type":"Person","name":"Olivier MF Martin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://omfmartin.com/es/posts/2024-02-18-explainable-ai/"},"publisher":{"@type":"Organization","name":"Olivier MF Martin","logo":{"@type":"ImageObject","url":"https://omfmartin.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://omfmartin.com/es/ accesskey=h title="Olivier MF Martin (Alt + H)">Olivier MF Martin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://omfmartin.com/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://omfmartin.com/es/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://omfmartin.com/es/sobre-mi title="Sobre Mí"><span>Sobre Mí</span></a></li><li><a href=https://omfmartin.com/es/buscador title="Buscador (Alt + /)" accesskey=/><span>Buscador</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Una Guía Rápida sobre la IA Explicable</h1><div class=post-meta><span title='2024-02-18 00:00:00 +0000 UTC'>18/02/2024</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Olivier MF Martin&nbsp;|&nbsp;Traducciones:<ul class=i18n_list><li><a href=https://omfmartin.com/posts/2024-02-18-explainable-ai/>English</a></li></ul></div></header><div class=post-content><h2 id=qué-es-la-ia-explicable>¿Qué es la IA Explicable?<a hidden class=anchor aria-hidden=true href=#qué-es-la-ia-explicable>#</a></h2><p>Cada año, la inteligencia artificial (IA) y los modelos de aprendizaje automático se vuelven más complejos, gracias a ordenadores más rápidos, algoritmos más ágiles y más datos. Ciertos modelos ahora se componen de cientos de miles de millones de parámetros. Esta complejidad les hace difíciles, si no imposibles, de entender.</p><figure><img style=margin:auto src=/posts/2024-02-18-explainable-ai/increasing_number_of_parameters.png alt="El aumento del número de parámetros en modelos de IA entre 1954 y 2024."><figcaption>El aumento en el número de parámetros del modelo entre 1954 y 2024. Datos de <a href=https://towardsdatascience.com/parameter-counts-in-machine-learning-a312dc4753d0>Jaime Sevilla</a>.</figcaption></figure><p>Aquí es donde entra en escena la IA explicable (XAI, por sus siglas en inglés). La IA explicable busca abrir cajas negras, proporcionando <em>insights</em> sobre cómo y por qué los algoritmos hacen sus predicciones. Al permitirnos explorar cómo las entradas de datos influyen en las predicciones del modelo, la IA explicable hace que la IA sea más transparente, fomenta la confianza y permite que tanto los desarrolladores como los usuarios finales comprendan el razonamiento del modelo.</p><p>Este artículo ofrece una breve introducción a la IA explicable, destacando su importancia y las metodologías involucradas. Para aquellos que busquen un análisis más profundo del tema, la sección de recursos al final de este artículo ofrece una selección de libros y herramientas de software dedicadas a IA explicable.</p><h2 id=casos-de-uso-para-abrir-la-caja-negra>Casos de Uso para Abrir la Caja Negra<a hidden class=anchor aria-hidden=true href=#casos-de-uso-para-abrir-la-caja-negra>#</a></h2><p><img loading=lazy src=/posts/2024-02-18-explainable-ai/opening_the_black_box.png alt="Una persona se encuentra en una habitación tenuemente iluminada, su silueta delineada por el suave resplandor que emana de una misteriosa caja negra abierta que sostiene en sus manos."></p><h3 id=depuración-y-mejora-de-modelos>Depuración y Mejora de Modelos<a hidden class=anchor aria-hidden=true href=#depuración-y-mejora-de-modelos>#</a></h3><p>Los modelos predictivos cometen errores. Comprender las causas raíz que conducen a estas inexactitudes es esencial para mejorar el rendimiento del modelo. Al ayudarnos a entender el razonamiento detrás de las predicciones, las técnicas de IA explicable nos ayudan a caracterizar las situaciones donde el modelo está cometiendo errores. Esto hace de la IA explicable una herramienta valiosa para la depuración de modelos y contribuye al refinamiento del modelo.</p><p>Consideremos un escenario que involucra una tarea de clasificación de animales. Las técnicas de IA explicable, como los <a href=https://en.wikipedia.org/wiki/Saliency_map>mapas de saliencia</a>, pueden resaltar las partes de una imagen que el clasificador considera significativas para hacer una predicción. Este análisis podría revelar que, en el caso de una imagen de un camello, el modelo erróneamente se concentra en la arena en lugar del camello en sí. Esto ayudaría a explicar por qué el modelo es incapaz de clasificar con precisión imágenes de camellos en otros entornos.</p><h3 id=detección-y-mitigación-de-sesgos>Detección y Mitigación de Sesgos<a hidden class=anchor aria-hidden=true href=#detección-y-mitigación-de-sesgos>#</a></h3><p>Los modelos predictivos pueden estar sesgados, lo que conduce a desigualdades sociales y perjudica a los grupos minoritarios. Los sesgos algorítmicos suelen provenir del uso de datos sesgados o incompletos, o de las suposiciones integradas en los modelos.</p><p>Un ejemplo notable de tal sesgo se observó en un <a href=https://www.science.org/doi/10.1126/science.aax2342#supplementary-materials>algoritmo comercial utilizado por el sistema de salud de EE.UU.</a>. Específicamente, para idénticas puntuaciones de riesgo predichas, se encontró que los pacientes negros estaban más enfermos que sus homólogos blancos. Este sesgo provino del modelo que suponía que mayores costos de atención médica significaban mayores necesidades de atención médica. Dado que se gastaba menos dinero en la población negra, el algoritmo concluyó erróneamente que corrían menos riesgo.</p><p>Instancias similares de sesgo algorítmico han sido documentadas en el <a href=https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm>sistema judicial</a> y <a href=https://www.bbc.com/news/technology-45809919>procesos de solicitud de empleo</a>. Estos ejemplos destacan la necesidad de responsabilidad en los sistemas de IA, especialmente cuando se despliegan en áreas con impactos sociales significativos.</p><p>Las técnicas de IA explicable pueden ayudar a identificar estos sesgos contextualizando por qué los modelos hacen ciertas predicciones. Por ejemplo, en el ejemplo de atención médica mencionado anteriormente, la IA explicable podría demostrar cómo ser negro bajaba la puntuación de riesgo. Por lo tanto, la IA explicable es instrumental no solo en la interpretación de predicciones de modelos sino también en impulsar el desarrollo de sistemas de IA más equitativos.</p><h2 id=escenarios-de-alto-riesgo>Escenarios de Alto Riesgo<a hidden class=anchor aria-hidden=true href=#escenarios-de-alto-riesgo>#</a></h2><p>Predicciones erróneas o sesgadas pueden impactar profundamente las vidas y el bienestar de los individuos, especialmente cuando se utilizan en escenarios de alto riesgo. Por ejemplo, al usar algoritmos predictivos para sistemas de puntuación social, categorización individual, tecnologías de reconocimiento facial, herramientas de aplicación de la ley, procesos de selección de empleo y la provisión de servicios esenciales como la atención sanitaria.</p><p>En la práctica, la explicación puede ser tan importante como la predicción. Por esta razón, en sectores de alto riesgo como la atención sanitaria, finanzas y justicia penal, hay una marcada preferencia por emplear modelos inherentemente interpretables. Por ejemplo, la regresión logística, con su interpretabilidad y simplicidad, a menudo se prefiere sobre redes neuronales más complejas dentro de los entornos de atención sanitaria.</p><h3 id=cumplimiento-regulatorio>Cumplimiento Regulatorio<a hidden class=anchor aria-hidden=true href=#cumplimiento-regulatorio>#</a></h3><p>A medida que el despliegue de la IA se vuelve más generalizado, las leyes y regulaciones están comenzando a requerir que las decisiones de IA sean transparentes y explicables. Este cambio tiene como objetivo garantizar que estos algoritmos operen dentro de límites éticos.</p><p>Por ejemplo, el Reglamento General de Protección de Datos (GDPR) de la Unión Europea exige una transparencia significativa sobre la lógica involucrada en las decisiones automatizadas, particularmente en decisiones que afectan significativamente a los individuos, como aquellas relacionadas con el empleo, la solvencia y asuntos legales. Esta regulación ha sido interpretada por algunos como un <em>derecho a la explicación</em>. Basándose en esta base, el más reciente <em>AI Act</em> ha introducido disposiciones específicas para algoritmos de &ldquo;alto riesgo&rdquo;, con requisitos relacionados con la &ldquo;transparencia y provisión de información a los usuarios&rdquo;.</p><p>Sin embargo, estas regulaciones enfrentan desafíos en la aplicación e interpretación. La complejidad inherente de los algoritmos de IA puede hacer que sea desafiante proporcionar explicaciones que sean técnicamente precisas y fácilmente comprensibles para el público general. A pesar de estos desafíos, la presión por tales regulaciones destaca la creciente preocupación por la transparencia en las aplicaciones de IA y subraya la necesidad crítica de la IA explicable.</p><h3 id=descubrimiento-de-conocimiento>Descubrimiento de Conocimiento<a hidden class=anchor aria-hidden=true href=#descubrimiento-de-conocimiento>#</a></h3><p>El uso de la IA explicable para generar nuevo conocimiento es probablemente la aplicación más pasada por alto. Esta capacidad es particularmente interesante dado que los modelos de aprendizaje automático son más aptos para identificar patrones y tendencias complejas en los datos en comparación con pruebas estadísticas tradicionales como las pruebas t o chi-cuadrado. Este enfoque fue sugerido para <a href=https://doi.org/10.1007/s00441-023-03816-z>el descubrimiento de biomarcadores</a>.</p><p>A pesar de su potencial, emplear la IA explicable para la generación de conocimiento no está exento de desafíos. Algunos de los problemas más significativos incluyen:</p><ul><li><strong>Ambigüedad del Umbral de Significancia</strong>: A diferencia de los métodos estadísticos tradicionales, que tienen criterios bien definidos para la significancia estadística, la IA explicable carece de un marco estandarizado para determinar el nivel de significancia de las variables.</li><li><strong>Limitaciones de Datos Observacionales</strong>: La mayoría de los datos utilizados en el aprendizaje automático son observacionales, lo que es inherentemente menos efectivo para establecer relaciones causales.</li><li><strong>Sobreajuste</strong>: Existe el riesgo de que los modelos confundan el ruido aleatorio con patrones significativos, lo que potencialmente podría llevar a interpretaciones y conclusiones incorrectas sobre los datos.</li></ul><h3 id=cuándo-podemos-prescindir-de-una-explicación>¿Cuándo podemos prescindir de una explicación?<a hidden class=anchor aria-hidden=true href=#cuándo-podemos-prescindir-de-una-explicación>#</a></h3><p>No todas las predicciones hechas por la IA necesitan ser explicadas. Generar explicaciones introduce complejidad adicional y necesidades computacionales, que se traducen en gastos financieros e impacto ambiental. Implementar IA explicable puede ser injustificado en ciertos escenarios, tales como:</p><ul><li><strong>Escenarios de Bajo Riesgo</strong>: En situaciones donde las consecuencias de los errores son mínimas, como las recomendaciones en una plataforma de streaming. Los usuarios típicamente tienen poco interés en entender el funcionamiento interno de cómo se generan estas sugerencias.</li><li><strong>Problemas Bien Entendidos</strong>: Para problemas bien establecidos como el filtrado de spam en sistemas de correo electrónico, las explicaciones para cada decisión pueden no ser esenciales porque la fiabilidad y precisión general del sistema ya son bien comprendidas.</li><li><strong>Riesgos de Manipulación</strong>: En algunos casos, proporcionar explicaciones detalladas puede ayudar involuntariamente a aquellos que buscan explotar el sistema. Por ejemplo, los expertos en SEO podrían malusar información detallada sobre algoritmos de búsqueda para manipular los rankings y socavar la integridad del sistema.</li></ul><h2 id=enfoques-para-explicar-la-ia>Enfoques para Explicar la IA<a hidden class=anchor aria-hidden=true href=#enfoques-para-explicar-la-ia>#</a></h2><p>Los intrincados detalles matemáticos y cálculos de un algoritmo usualmente no revelan mucho sobre cómo toma decisiones. Esto se debe a que las representaciones aprendidas por el modelo predictivo típicamente están más allá de la capacidad de comprensión del cerebro humano. Debido a esto, los enfoques directos, como divulgar el código fuente o los cálculos matemáticos, no logran proporcionar comprensión.</p><p>Para superar este desafío, las técnicas de IA explicable utilizan modelos inherentemente interpretables o aproximan modelos complejos de caja negra con otros más simples y comprensibles. La IA explicable es un campo activo de investigación. Actualmente, no existe una manera universal de explicar algoritmos predictivos complejos, y podría nunca haberla.</p><p>Nuestro enfoque será en categorizar las técnicas utilizadas para explicar modelos predictivos, destacando algunos de los métodos más comúnmente empleados.</p><p><img loading=lazy src=/posts/2024-02-18-explainable-ai/classification_of_xai.png alt="Una representación visual abstracta de una clasificación de técnicas de IA explicable."></p><h3 id=etapa-ante-hoc-vs-post-hoc>Etapa: Ante-hoc vs Post-hoc<a hidden class=anchor aria-hidden=true href=#etapa-ante-hoc-vs-post-hoc>#</a></h3><ul><li><strong>Ante-hoc (Interpretabilidad Intrínseca)</strong>: Este enfoque se centra en el uso de modelos que son inherentemente interpretables debido a su estructura simple y procesos de toma de decisiones transparentes. Ejemplos incluyen modelos lineales, árboles de decisión y sistemas basados en reglas. Aunque estos modelos ofrecen claridad desde el principio, su interpretabilidad puede disminuir a medida que la complejidad o el número de parámetros aumenta. Actualmente hay investigaciones para diseñar modelos de alto rendimiento pero inherentemente interpretables; sin embargo, hasta ahora no han tenido mucho éxito.</li><li><strong>Post-hoc (Interpretabilidad Extrínseca)</strong>: Los métodos post-hoc tienen como objetivo explicar decisiones tomadas por modelos complejos, cajas negras, después de haber sido entrenados. Estas explicaciones no están integradas en el modelo sino que se derivan de analizar el comportamiento del modelo. Este enfoque es particularmente útil para modelos de cajas negras, como las redes neuronales profundas. Las explicaciones post-hoc buscan ser localmente fieles, lo que significa que reflejan con precisión el razonamiento del modelo para instancias o decisiones específicas.</li></ul><h3 id=modelo-específico-del-modelo-vs-agnóstico-del-modelo>Modelo: Específico del Modelo vs Agnóstico del Modelo<a hidden class=anchor aria-hidden=true href=#modelo-específico-del-modelo-vs-agnóstico-del-modelo>#</a></h3><ul><li><strong>Específico del Modelo</strong>: Estas técnicas solo son aplicables a tipos específicos de modelos, explotando sus arquitecturas únicas para proporcionar explicaciones. Para los modelos de aprendizaje profundo, técnicas como los Mapas de Saliencia, DeepLIFT y los Mapas de Activación de Clase (CAM) ilustran cómo diferentes características influencian las predicciones del modelo.</li><li><strong>Agnóstico del Modelo</strong>: Estas técnicas ofrecen una alternativa flexible. Los métodos agnósticos del modelo no requieren conocimiento del funcionamiento interno del modelo, haciéndolos aplicables a una amplia gama de sistemas de IA. Técnicas como <a href=https://doi.org/10.48550/arXiv.1705.07874>SHAP</a> (SHapley Additive exPlanations) y <a href=https://github.com/marcotcr/lime>LIME</a> (Explicaciones Locales Interpretables Agnósticas del Modelo) pueden usarse para explicar cualquier modelo aproximando su límite de decisión o salida de una manera interpretable.</li></ul><h3 id=alcance-global-vs-local>Alcance: Global vs Local<a hidden class=anchor aria-hidden=true href=#alcance-global-vs-local>#</a></h3><ul><li><strong>Explicaciones Locales</strong>: Estas se centran en predicciones o decisiones individuales, proporcionando <em>insights</em> sobre por qué el modelo tomó una elección particular para una instancia específica. Herramientas como SHAP y LIME ofrecen explicaciones locales y muestran la influencia de varias características en un único resultado.</li><li><strong>Explicaciones Globales</strong>: En contraste, las explicaciones globales buscan mostrar el comportamiento del modelo a través del conjunto de datos completo o en un sentido más general. Técnicas para lograr explicaciones globales incluyen rankings de importancia de características y gráficas de dependencia parcial, que agregan los efectos de las características a través de múltiples instancias para ofrecer una comprensión más amplia de la lógica del modelo.</li></ul><h2 id=las-limitaciones-de-la-ia-explicable>Las Limitaciones de la IA explicable<a hidden class=anchor aria-hidden=true href=#las-limitaciones-de-la-ia-explicable>#</a></h2><p>A medida que la IA continúa permeando varios aspectos de nuestras vidas, el imperativo de transparencia y comprensibilidad en los sistemas de IA nunca ha sido más pronunciado. Sin embargo, como en cualquier dominio en rápida evolución, la IA explicable no está exento de desafíos y limitaciones.</p><p><img loading=lazy src=/posts/2024-02-18-explainable-ai/complex_ai.png alt="La imagen muestra a una persona sentada frente a una máquina masiva y compleja, compuesta por innumerables engranajes, palancas y pantallas, simbolizando una IA compleja."></p><h3 id=el-_tradeoff_-entre-complejidad-y-rendimiento>El <em>tradeoff</em> entre complejidad y rendimiento<a hidden class=anchor aria-hidden=true href=#el-_tradeoff_-entre-complejidad-y-rendimiento>#</a></h3><p>El <em>tradeoff</em> entre complejidad y rendimiento sugiere que mejorar el rendimiento de los modelos de IA generalmente requiere aumentar su complejidad. Esta suposición subraya la necesidad percibida de equilibrar la complejidad contra el rendimiento. Sin embargo, esta perspectiva merece un examen crítico. La <a href=https://doi.org/10.48550/arXiv.1803.03635>hipótesis del boleto de lotería</a> propone que la eficacia de los modelos grandes podría derivarse de una inicialización óptima de parámetros en lugar de su mero tamaño. Empíricamente, se ha demostrado que modelos más simples pueden alcanzar un rendimiento comparable al de sus contrapartes más complejas.</p><p>De hecho, la evidencia muestra que modelos con menos parámetros, como Mistral-7B, pueden alcanzar niveles de rendimiento comparables a los de sus contrapartes más complejas, como GPT-3.5, con una fracción de los parámetros (7 mil millones vs. 175 mil millones). Esta observación desafía la necesidad de modelos excesivamente complejos, abogando por un enfoque más medido en el diseño de modelos donde la simplicidad no excluye la efectividad.</p><p><a href=https://doi.org/10.1038%2Fs42256-019-0048-x>Algunos investigadores</a> argumentan que los modelos no inherentemente interpretables deberían evitarse por completo en escenarios de alto riesgo. Sin embargo, sigue sin estar claro si esto es factible en todos los contextos sin sacrificar el rendimiento.</p><h3 id=explicando-la-explicación>Explicando la Explicación<a hidden class=anchor aria-hidden=true href=#explicando-la-explicación>#</a></h3><p>Para una implementación efectiva de las técnicas de IA explicable, es crucial entender quién es el público objetivo de las explicaciones. La IA explicable sirve principalmente a profesionales técnicos, como científicos de datos e ingenieros de aprendizaje automático, que tienen un profundo entendimiento de estadística.</p><p>Sin embargo, existe una distinción significativa entre las explicaciones adecuadas para expertos en IA y aquellas para usuarios no técnicos. Los detalles técnicos pueden abrumar a los usuarios no técnicos, llevando a la confusión o malinterpretación. Por ejemplo, alguien con conocimiento limitado de estadística podría interpretar erróneamente los valores SHAP como indicativos de causalidad, en lugar de mera correlación.</p><p>Reconocer el conocimiento, objetivos, habilidades y capacidades del usuario final es primordial al usar la IA explicable. Además, la comunicación de las explicaciones de IA debe estar alineada con el nivel de comprensión de la audiencia. Esto es necesario para evitar tanto los riesgos de simplificación excesiva, que pueden llevar a malentendidos, como la ofuscación, que puede alienar y frustrar a los usuarios.</p><h3 id=fidelidad-y-los-límites-de-la-explicación>Fidelidad y los Límites de la Explicación<a hidden class=anchor aria-hidden=true href=#fidelidad-y-los-límites-de-la-explicación>#</a></h3><p>Las explicaciones post-hoc de modelos complejos se logran aproximando localmente su comportamiento alrededor de un punto de datos específico utilizando un modelo más simple y transparente. Aunque estas aproximaciones ofrecen <em>insights</em>, no logran capturar completamente el comportamiento del modelo original. Además, los métodos de IA explicable a menudo carecen de métricas para evaluar la calidad de estas aproximaciones, planteando preguntas sobre la fidelidad de tales explicaciones.</p><p>Adicionalmente, a medida que la complejidad de los modelos aumenta anualmente, nuestra capacidad para explicarlos con precisión disminuye. Esta creciente complejidad destaca una pregunta crucial: ¿Existe un límite teórico a nuestra capacidad para hacer transparentes los procesos de toma de decisiones de la IA? Y si tal límite existe, ¿cuán cerca estamos de alcanzarlo?</p><h2 id=conclusión>Conclusión<a hidden class=anchor aria-hidden=true href=#conclusión>#</a></h2><p>La IA explicable juega un papel crítico en abordar la opacidad de los complejos modelos de aprendizaje automático, permitiendo confiar en las decisiones basadas en IA. La IA facilita identificar sesgos, depurar modelos, cumplir con regulaciones y generar nuevo conocimiento explicando el comportamiento del modelo. Existen numerosos recursos de código abierto disponibles para la implementación fácil y eficiente de IA explicable. No obstante, es importante reconocer las limitaciones y desafíos asociados con IA explicable y favorecer modelos más simples siempre que sea posible.</p><h2 id=recursos>Recursos<a hidden class=anchor aria-hidden=true href=#recursos>#</a></h2><h3 id=libros>Libros<a hidden class=anchor aria-hidden=true href=#libros>#</a></h3><ul><li><a href=https://ema.drwhy.ai/>Análisis de Modelos Explicativos</a></li><li><a href=https://christophm.github.io/interpretable-ml-book/>Aprendizaje Automático Interpretable</a></li></ul><h3 id=software>Software<a hidden class=anchor aria-hidden=true href=#software>#</a></h3><ul><li><a href=https://shap.readthedocs.io/en/latest/>SHAP</a></li><li><a href=https://github.com/marcotcr/lime>LIME</a></li><li><a href=https://github.com/ModelOriented/DALEX>DALEX</a></li><li><a href=https://xaitk.org/>XAITK</a></li><li><a href=https://interpret.ml/>InterpretML</a></li></ul><h2 id=bibliografía>Bibliografía<a hidden class=anchor aria-hidden=true href=#bibliografía>#</a></h2><ul><li><p><a href=https://dx.doi.org/10.2139/ssrn.3901732>Ebers. Regulating Explainable AI in the European Union. An Overview of the Current Legal Framework(s). (2021)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.1803.03635>Frankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. (2018)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.2112.01016>Jiang and Senge. On Two XAI Cultures: A Case Study of Non-technical Explanations in Deployed AI System. (2021)</a></p></li><li><p><a href=https://doi.org/10.1002/ail2.61>Gunning et al. DARPA&rsquo;s explainable AI (XAI) program: A retrospective. (2021)</a></p></li><li><p><a href=https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm>Larson et al. How We Analyzed the COMPAS Recidivism Algorithm. (2016)</a></p></li><li><p><a href=https://doi.org/10.3390/e23010018>Linardatos et al. Explainable AI: A Review of Machine Learning Interpretability Methods. (2020)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.1705.07874>Lungberg and Lee. A Unified Approach to Interpreting Model Predictions. (2017)</a></p></li><li><p><a href=https://doi.org/10.1007/s00441-023-03816-z>Ng et al. The benefits and pitfalls of machine learning for biomarker discovery. (2023)</a></p></li><li><p><a href=https://doi.org/10.1126/science.aax2342>Obermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. (2019)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.1602.04938>Ribeiro et al. &ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier. (2016)</a></p></li><li><p><a href=https://doi.org/10.1038%2Fs42256-019-0048-x>Rudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. (2019)</a></p></li><li><p><a href=https://doi.org/10.48550/arXiv.2205.10119>Sarkar. Is explainable AI a race against model complexity? (2022)</a></p></li><li><p><a href=https://towardsdatascience.com/parameter-counts-in-machine-learning-a312dc4753d0>Sevilla. Parameter counts in Machine Learning. (2021)</a></p></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/><span class=title>« Anterior</span><br><span>Entender y Mitigar las Alucinaciones de los LLMs</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://omfmartin.com/es/>Olivier MF Martin</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copiar";function s(){t.innerHTML="¡copiado!",setTimeout(()=>{t.innerHTML="copiar"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>