[{"content":"Los Modelos de Lenguaje de Gran Tamaño (LLMs) inventan cosas. Ocasionalmente, estos modelos generarán contenido que, aunque parezca totalmente plausible, es fundamentalmente incorrecto. Este fenómeno se conoce como \u0026ldquo;alucinaciones\u0026rdquo;.\nLas alucinaciones pueden tener consecuencias desastrosas. Pueden difundir información errónea, potencialmente ofender a usuarios, e incluso comprometer información sensible. Este problema es un gran inconveniente de los LLMs y es la razón por la que muchos son cautelosos a la hora de emplearlos en entornos del mundo real.\nEn este artículo, discutiremos qué son las alucinaciones, explicaremos por qué ocurren y exploraremos estrategias para detectarlas y mitigarlas.\n¿Qué son las Alucinaciones? Las alucinaciones pueden definirse de manera general como respuestas que son factualmente incorrectas, infieles a la información proporcionada, o sin sentido. La mayoría de las veces, el modelo parecerá bastante seguro y su respuesta puede incluso parecer totalmente plausible. Esto hace que detectar alucinaciones sea una tarea difícil. Generalmente, las alucinaciones se dividen en dos categorías: intrínsecas y extrínsecas.\nLas alucinaciones intrínsecas ocurren cuando la respuesta del modelo contradice directamente la información que se le dio. Por ejemplo:\nPrompt: La capital de Francia es París. ¿Cuál es la capital de Francia? Respuesta: La capital de Francia es Lyon.\nLas alucinaciones extrínsecas se producen cuando el modelo realiza afirmaciones que no pueden ser verificadas ni contradichas con la información proporcionada. Por ejemplo:\nPrompt: Describe el impacto económico de las fuentes de energía renovable. Respuesta: Según un informe de 2045, las fuentes de energía renovable han llevado a la creación de 5 millones de nuevos empleos en todo el mundo.\n¿Por qué los LLMs Alucinan? Se considera que las alucinaciones en los modelos de lenguaje son inevitables. Pueden ocurrir debido a una variedad de factores, los cuales se pueden clasificar de manera general en problemas relacionados con los datos, el proceso de entrenamiento y la inferencia.\nCausas Relacionadas con los Datos Inexactitudes en los Datos de Entrenamiento: Los datos de entrenamiento son la base de los LLMs. Sin embargo, los datos no siempre son de la mejor calidad y pueden ser inexactos o sesgados. Dada la vasta cantidad de datos procesados, asegurar su precisión y relevancia es un desafío. Esto implica que parte de la información aprendida por el modelo lo llevará a generar respuestas engañosas o incorrectas. Consultas Fuera de Distribución: Los LLMs se entrenan en conjuntos de datos que cubren ciertos temas, períodos de tiempo y tipos de información. Cuando se enfrentan a preguntas o tareas que se encuentran fuera de estos límites, un modelo de lenguaje puede generar respuestas incorrectas. Por ejemplo, preguntar sobre las últimas noticias resultará en respuestas basadas en información desactualizada o irrelevante. Problemas de Utilización de Datos: Cómo los modelos de lenguaje utilizan su base de conocimientos también puede contribuir a las alucinaciones. Los modelos de lenguaje tienden a depender de patrones de co-ocurrencia de palabras para generar respuestas. Esta dependencia podría llevar a conclusiones incorrectas, como identificar a Sídney como la capital de Australia debido a su asociación más frecuente en comparación con la respuesta correcta, Canberra. Causas Relacionadas con el Entrenamiento Sesgo de Exposición: Este problema surge de una discrepancia entre cómo se entrena al modelo y el uso real del modelo. Durante el entrenamiento, el modelo aprende a predecir el siguiente token confiando en la secuencia correcta anterior de los datos de entrenamiento, un método conocido como \u0026ldquo;enseñanza forzada\u0026rdquo;. Sin embargo, durante la inferencia, el modelo depende de sus propias predicciones. Esto puede llevar a una acumulación de errores o a un efecto bola de nieve. Limitaciones Arquitectónicas: Los modelos de lenguaje causal generan texto prediciendo el siguiente token basándose únicamente en los tokens precedentes. Este diseño limita la capacidad del modelo para captar algunas sutilezas contextuales, lo que se cree que contribuye a la generación de contenido alucinado. Desalineación de Creencias: Los LLM generalmente pasan por un proceso de entrenamiento en dos etapas. Inicialmente, son preentrenados para adquirir un conocimiento amplio. Posteriormente, son ajustados (fine-tuned) para alinearse mejor con instrucciones o indicaciones específicas de los usuarios, a menudo utilizando técnicas como el aprendizaje por refuerzo. Este ajuste podría llevar inadvertidamente al modelo a priorizar las preferencias del usuario sobre la exactitud. Causas Relacionadas con la Inferencia Aleatoriedad en el Muestreo: Las estrategias de descodificación de los modelos de lenguaje a menudo incorporan cierta aleatoriedad para favorecer la creatividad y la calidad de la respuesta, por ejemplo, ajustando el parámetro de temperatura para favorecer tokens menos probables. Este enfoque puede aumentar inadvertidamente la probabilidad de generar contenido sin sentido. Cuello de Botella de Softmax: La última capa de un LLM estima la probabilidad del siguiente token. La mayoría de los LLM utilizan una capa softmax, que se sabe que tiene una limitación llamada el cuello de botella de softmax. Esta limitación impide la capacidad del modelo para distinguir eficazmente entre resultados altamente probables. Atención Diluida: Con secuencias de entrada más largas, la eficiencia del mecanismo de atención del modelo puede disminuir, lo que lleva a una atención debilitada en detalles y contexto esenciales. Esta dilución de la atención compromete la capacidad del modelo para mantenerse coherente y preciso en textos extendidos. ¿Cómo podemos detectar las alucinaciones? Detectar alucinaciones presenta desafíos significativos que las métricas estándar utilizadas en el Procesamiento de Lenguaje Natural (NLP) como BLEU, ROUGE, o BERTScore no solucionan. No solo estas métricas no predicen con precisión las alucinaciones, sino que también dependen de la respuesta de referencia, lo cual es impracticable para aplicaciones generalizadas.\nUn enfoque para la detección de alucinaciones es emplear métricas probabilísticas que calculan la probabilidad de alucinaciones basadas en probabilidades de tokens o medidas de entropía, por ejemplo, estimando la perplejidad. Sin embargo, estas métricas requieren acceso a las probabilidades de los tokens, que pueden no estar disponibles, lo que limita su adopción.\nUna estrategia más efectiva ha sido el desarrollo de métricas basadas en modelos. Este enfoque implica utilizar otro modelo de lenguaje, por ejemplo, un LLM de última generación, para evaluar la probabilidad de contenido alucinatorio. Estas métricas se pueden clasificar aún más en métricas a nivel de token y a nivel de frase.\nLas métricas a nivel de token intentan predecir para cada token si fue alucinado o no. Para implementar esta estrategia, los investigadores generalmente generan un conjunto de datos alucinados sintéticamente aplicando perturbaciones a conjuntos de datos de referencia libres de alucinaciones. Luego se entrena un clasificador de lenguaje binario, como uno basado en BERT o XLNet, para discernir entre tokens alucinados y no alucinados. Proyectos que utilizan este enfoque son HADES y el pipeline de detección de alucinaciones fairseq-detect-hallucination.\nPor otro lado, las métricas a nivel de frase evalúan la oración completa para determinar la presencia de alucinaciones. Un método destacado es SelfCheckGPT, una técnica basada en el muestreo que se fundamenta en la suposición de que las alucinaciones son más probables de ocurrir cuando el modelo está incierto. Este método evalúa la autoconsistencia comparando varias respuestas muestreadas, utilizando métricas estándar como coincidencia de N-gramas o BERTScore, para estimar la probabilidad de contenido alucinatorio. Una fortaleza de este método es que no requiere ningún dato externo, lo que facilita su escalabilidad.\nEstrategias para Mitigar las Alucinaciones Las estrategias de mitigación de alucinaciones tienen como objetivo reducir la probabilidad de que un LLM genere contenido alucinatorio. A continuación, exploramos varias técnicas, ordenadas desde ajustes sencillos hasta intervenciones más complejas.\n1. Ajustar los Parámetros de Inferencia Uno de los métodos más sencillos para reducir las alucinaciones es ajustar la configuración operativa del LLM. Por ejemplo, afinar parámetros como la temperatura de generación y el muestreo top-k puede regular la aleatoriedad de la respuesta del LLM. Valores más bajos de temperatura o top-k pueden producir respuestas más cautelosas y conservadoras, minimizando potencialmente las salidas alucinatorias.\n2. Ingeniería de Prompts La ingeniería de indicaciones efectiva es crucial para dirigir los LLM hacia respuestas precisas. Los siguientes consejos generales son esenciales para optimizar los prompts:\nClaridad y Precisión: Asegurar que el prompt sea claro sobre el tipo exacto de información requerida.\nReconocimiento de Incertidumbre: Solicitar explícitamente que el modelo evite inventar información y exprese incertidumbre cuando sea aplicable, por ejemplo diciendo \u0026ldquo;No lo sé\u0026rdquo;.\nÉnfasis en la Información Clave: Repetir detalles importantes dentro del prompt puede ayudar a subrayar la importancia de estos elementos. Esto ayuda a asegurar que reciban la debida atención en la respuesta del modelo.\nColocación Estratégica de la Información: Posicionar la información más crucial hacia el final del prompt puede mejorar la atención del modelo en estos detalles clave.\nEspecificación del Formato de Salida: Dirigir al modelo para que estructure su respuesta en un formato específico puede ayudar a obtener información que está organizada de manera que mejor se adapte a las necesidades del usuario.\nAdemás, técnicas más avanzadas de ingeniería de prompts ofrecen capas adicionales de sofisticación:\nIndicaciones Múltiples: involucra proporcionar al modelo múltiples ejemplos del resultado deseado. Este enfoque ayuda al modelo a comprender mejor el contexto y alinear sus respuestas con el formato de respuesta esperado.\nCadena de Pensamiento: indica al modelo a mostrar su proceso de razonamiento paso a paso, ofreciendo a los usuarios percepciones sobre la progresión lógica que lleva a la respuesta.\nAutoconsistencia: implica generar varias respuestas y seleccionar la que demuestre el mayor grado de consistencia interna, mejorando así la fiabilidad.\nCadena de Verificación: es una técnica donde el modelo se utiliza para verificar sus respuestas o hacer anotaciones, mejorando aún más la precisión y la confiabilidad de la información proporcionada.\n3. Técnicas Basadas en Recuperación Estas técnicas mejoran las respuestas del modelo incorporando fuentes de conocimiento externas y contexto a la indicación. Probablemente, la técnica más conocida es Generación Aumentada por Recuperación (RAG, por sus siglas en inglés): Esta técnica codifica documentos externos utilizando sentence embeddings, generalmente mediante alguna red neuronal basada en transformadores como BERT. Las embeddings resultantes se almacenan luego en una base de datos vectorial, que luego se puede consultar de manera eficiente usando el prompt original (o alguna variación). Los datos recuperados se utilizan entonces para enriquecer el prompt. Una implementación popular es FAISS. El principal inconveniente de estos enfoques basados en recuperación es que requieren mantener una base de datos grande y curada.\n4. Ajuste Fino de LLM La calidad de la salida de un modelo depende directamente de sus datos de entrenamiento. Ajustar un LLM con datos actualizados o más relevantes puede abordar debilidades o lagunas específicas en su base de conocimiento. Aunque volver a entrenar desde cero es a menudo impracticable, el ajuste fino ofrece una alternativa rentable para mejorar la precisión del modelo en aplicaciones específicas. Técnicas como LoRA ayudan a reducir el número de parámetros entrenables lo que acelera el ajuste fino haciéndolo una opción viable.\nReflexión Final Hay una línea fina entre la creatividad y las alucinaciones. Por un lado, un modelo que se inclina demasiado hacia la precaución puede evitar las alucinaciones pero a costa de producir respuestas que son poco interesantes o demasiado simplistas. Por otro lado, un modelo excesivamente creativo producirá salidas que se asemejan a una entrevista con Salvador Dalí.\nEncontrar el equilibrio correcto es clave para aprovechar el potencial completo de los LLM mientras se minimizan los riesgos asociados con las alucinaciones. En última instancia, entender y mitigar las alucinaciones no es solo sobre prevenir errores; se trata de construir confianza entre los humanos y la inteligencia artificial, asegurando que estas herramientas mejoren nuestros procesos de toma de decisiones, creatividad y acceso a la información.\nBibliografía Huang et al. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. (2023) Ji et al. Survey of Hallucination in Natural Language Generation. (2022) Luo et al. Hallucination Detection and Hallucination Mitigation: An Investigation. (2024) Manakul et al. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. (2023) Xu et al. Hallucination is Inevitable: An Innate Limitation of Large Language Models. (2024) Zhou et al. Detecting Hallucinated Content in Conditional Neural Sequence Generation. (2020) ","permalink":"https://omfmartin.com/es/posts/2024-03-24-llm-hallucination/","summary":"¿Cómo podemos detectar y mitigar las alucinaciones para un despliegue más seguro de los LLMs?","title":"Entender y Mitigar las Alucinaciones de los LLMs"},{"content":"¿Qué es la IA Explicable? Cada año, la inteligencia artificial (IA) y los modelos de aprendizaje automático se vuelven más complejos, gracias a ordenadores más rápidos, algoritmos más ágiles y más datos. Ciertos modelos ahora se componen de cientos de miles de millones de parámetros. Esta complejidad les hace difíciles, si no imposibles, de entender.\nEl aumento en el número de parámetros del modelo entre 1954 y 2024. Datos de Jaime Sevilla. Aquí es donde entra en escena la IA explicable (XAI, por sus siglas en inglés). La IA explicable busca abrir cajas negras, proporcionando insights sobre cómo y por qué los algoritmos hacen sus predicciones. Al permitirnos explorar cómo las entradas de datos influyen en las predicciones del modelo, la IA explicable hace que la IA sea más transparente, fomenta la confianza y permite que tanto los desarrolladores como los usuarios finales comprendan el razonamiento del modelo.\nEste artículo ofrece una breve introducción a la IA explicable, destacando su importancia y las metodologías involucradas. Para aquellos que busquen un análisis más profundo del tema, la sección de recursos al final de este artículo ofrece una selección de libros y herramientas de software dedicadas a IA explicable.\nCasos de Uso para Abrir la Caja Negra Depuración y Mejora de Modelos Los modelos predictivos cometen errores. Comprender las causas raíz que conducen a estas inexactitudes es esencial para mejorar el rendimiento del modelo. Al ayudarnos a entender el razonamiento detrás de las predicciones, las técnicas de IA explicable nos ayudan a caracterizar las situaciones donde el modelo está cometiendo errores. Esto hace de la IA explicable una herramienta valiosa para la depuración de modelos y contribuye al refinamiento del modelo.\nConsideremos un escenario que involucra una tarea de clasificación de animales. Las técnicas de IA explicable, como los mapas de saliencia, pueden resaltar las partes de una imagen que el clasificador considera significativas para hacer una predicción. Este análisis podría revelar que, en el caso de una imagen de un camello, el modelo erróneamente se concentra en la arena en lugar del camello en sí. Esto ayudaría a explicar por qué el modelo es incapaz de clasificar con precisión imágenes de camellos en otros entornos.\nDetección y Mitigación de Sesgos Los modelos predictivos pueden estar sesgados, lo que conduce a desigualdades sociales y perjudica a los grupos minoritarios. Los sesgos algorítmicos suelen provenir del uso de datos sesgados o incompletos, o de las suposiciones integradas en los modelos.\nUn ejemplo notable de tal sesgo se observó en un algoritmo comercial utilizado por el sistema de salud de EE.UU.. Específicamente, para idénticas puntuaciones de riesgo predichas, se encontró que los pacientes negros estaban más enfermos que sus homólogos blancos. Este sesgo provino del modelo que suponía que mayores costos de atención médica significaban mayores necesidades de atención médica. Dado que se gastaba menos dinero en la población negra, el algoritmo concluyó erróneamente que corrían menos riesgo.\nInstancias similares de sesgo algorítmico han sido documentadas en el sistema judicial y procesos de solicitud de empleo. Estos ejemplos destacan la necesidad de responsabilidad en los sistemas de IA, especialmente cuando se despliegan en áreas con impactos sociales significativos.\nLas técnicas de IA explicable pueden ayudar a identificar estos sesgos contextualizando por qué los modelos hacen ciertas predicciones. Por ejemplo, en el ejemplo de atención médica mencionado anteriormente, la IA explicable podría demostrar cómo ser negro bajaba la puntuación de riesgo. Por lo tanto, la IA explicable es instrumental no solo en la interpretación de predicciones de modelos sino también en impulsar el desarrollo de sistemas de IA más equitativos.\nEscenarios de Alto Riesgo Predicciones erróneas o sesgadas pueden impactar profundamente las vidas y el bienestar de los individuos, especialmente cuando se utilizan en escenarios de alto riesgo. Por ejemplo, al usar algoritmos predictivos para sistemas de puntuación social, categorización individual, tecnologías de reconocimiento facial, herramientas de aplicación de la ley, procesos de selección de empleo y la provisión de servicios esenciales como la atención sanitaria.\nEn la práctica, la explicación puede ser tan importante como la predicción. Por esta razón, en sectores de alto riesgo como la atención sanitaria, finanzas y justicia penal, hay una marcada preferencia por emplear modelos inherentemente interpretables. Por ejemplo, la regresión logística, con su interpretabilidad y simplicidad, a menudo se prefiere sobre redes neuronales más complejas dentro de los entornos de atención sanitaria.\nCumplimiento Regulatorio A medida que el despliegue de la IA se vuelve más generalizado, las leyes y regulaciones están comenzando a requerir que las decisiones de IA sean transparentes y explicables. Este cambio tiene como objetivo garantizar que estos algoritmos operen dentro de límites éticos.\nPor ejemplo, el Reglamento General de Protección de Datos (GDPR) de la Unión Europea exige una transparencia significativa sobre la lógica involucrada en las decisiones automatizadas, particularmente en decisiones que afectan significativamente a los individuos, como aquellas relacionadas con el empleo, la solvencia y asuntos legales. Esta regulación ha sido interpretada por algunos como un derecho a la explicación. Basándose en esta base, el más reciente AI Act ha introducido disposiciones específicas para algoritmos de \u0026ldquo;alto riesgo\u0026rdquo;, con requisitos relacionados con la \u0026ldquo;transparencia y provisión de información a los usuarios\u0026rdquo;.\nSin embargo, estas regulaciones enfrentan desafíos en la aplicación e interpretación. La complejidad inherente de los algoritmos de IA puede hacer que sea desafiante proporcionar explicaciones que sean técnicamente precisas y fácilmente comprensibles para el público general. A pesar de estos desafíos, la presión por tales regulaciones destaca la creciente preocupación por la transparencia en las aplicaciones de IA y subraya la necesidad crítica de la IA explicable.\nDescubrimiento de Conocimiento El uso de la IA explicable para generar nuevo conocimiento es probablemente la aplicación más pasada por alto. Esta capacidad es particularmente interesante dado que los modelos de aprendizaje automático son más aptos para identificar patrones y tendencias complejas en los datos en comparación con pruebas estadísticas tradicionales como las pruebas t o chi-cuadrado. Este enfoque fue sugerido para el descubrimiento de biomarcadores.\nA pesar de su potencial, emplear la IA explicable para la generación de conocimiento no está exento de desafíos. Algunos de los problemas más significativos incluyen:\nAmbigüedad del Umbral de Significancia: A diferencia de los métodos estadísticos tradicionales, que tienen criterios bien definidos para la significancia estadística, la IA explicable carece de un marco estandarizado para determinar el nivel de significancia de las variables. Limitaciones de Datos Observacionales: La mayoría de los datos utilizados en el aprendizaje automático son observacionales, lo que es inherentemente menos efectivo para establecer relaciones causales. Sobreajuste: Existe el riesgo de que los modelos confundan el ruido aleatorio con patrones significativos, lo que potencialmente podría llevar a interpretaciones y conclusiones incorrectas sobre los datos. ¿Cuándo podemos prescindir de una explicación? No todas las predicciones hechas por la IA necesitan ser explicadas. Generar explicaciones introduce complejidad adicional y necesidades computacionales, que se traducen en gastos financieros e impacto ambiental. Implementar IA explicable puede ser injustificado en ciertos escenarios, tales como:\nEscenarios de Bajo Riesgo: En situaciones donde las consecuencias de los errores son mínimas, como las recomendaciones en una plataforma de streaming. Los usuarios típicamente tienen poco interés en entender el funcionamiento interno de cómo se generan estas sugerencias. Problemas Bien Entendidos: Para problemas bien establecidos como el filtrado de spam en sistemas de correo electrónico, las explicaciones para cada decisión pueden no ser esenciales porque la fiabilidad y precisión general del sistema ya son bien comprendidas. Riesgos de Manipulación: En algunos casos, proporcionar explicaciones detalladas puede ayudar involuntariamente a aquellos que buscan explotar el sistema. Por ejemplo, los expertos en SEO podrían malusar información detallada sobre algoritmos de búsqueda para manipular los rankings y socavar la integridad del sistema. Enfoques para Explicar la IA Los intrincados detalles matemáticos y cálculos de un algoritmo usualmente no revelan mucho sobre cómo toma decisiones. Esto se debe a que las representaciones aprendidas por el modelo predictivo típicamente están más allá de la capacidad de comprensión del cerebro humano. Debido a esto, los enfoques directos, como divulgar el código fuente o los cálculos matemáticos, no logran proporcionar comprensión.\nPara superar este desafío, las técnicas de IA explicable utilizan modelos inherentemente interpretables o aproximan modelos complejos de caja negra con otros más simples y comprensibles. La IA explicable es un campo activo de investigación. Actualmente, no existe una manera universal de explicar algoritmos predictivos complejos, y podría nunca haberla.\nNuestro enfoque será en categorizar las técnicas utilizadas para explicar modelos predictivos, destacando algunos de los métodos más comúnmente empleados.\nEtapa: Ante-hoc vs Post-hoc Ante-hoc (Interpretabilidad Intrínseca): Este enfoque se centra en el uso de modelos que son inherentemente interpretables debido a su estructura simple y procesos de toma de decisiones transparentes. Ejemplos incluyen modelos lineales, árboles de decisión y sistemas basados en reglas. Aunque estos modelos ofrecen claridad desde el principio, su interpretabilidad puede disminuir a medida que la complejidad o el número de parámetros aumenta. Actualmente hay investigaciones para diseñar modelos de alto rendimiento pero inherentemente interpretables; sin embargo, hasta ahora no han tenido mucho éxito. Post-hoc (Interpretabilidad Extrínseca): Los métodos post-hoc tienen como objetivo explicar decisiones tomadas por modelos complejos, cajas negras, después de haber sido entrenados. Estas explicaciones no están integradas en el modelo sino que se derivan de analizar el comportamiento del modelo. Este enfoque es particularmente útil para modelos de cajas negras, como las redes neuronales profundas. Las explicaciones post-hoc buscan ser localmente fieles, lo que significa que reflejan con precisión el razonamiento del modelo para instancias o decisiones específicas. Modelo: Específico del Modelo vs Agnóstico del Modelo Específico del Modelo: Estas técnicas solo son aplicables a tipos específicos de modelos, explotando sus arquitecturas únicas para proporcionar explicaciones. Para los modelos de aprendizaje profundo, técnicas como los Mapas de Saliencia, DeepLIFT y los Mapas de Activación de Clase (CAM) ilustran cómo diferentes características influencian las predicciones del modelo. Agnóstico del Modelo: Estas técnicas ofrecen una alternativa flexible. Los métodos agnósticos del modelo no requieren conocimiento del funcionamiento interno del modelo, haciéndolos aplicables a una amplia gama de sistemas de IA. Técnicas como SHAP (SHapley Additive exPlanations) y LIME (Explicaciones Locales Interpretables Agnósticas del Modelo) pueden usarse para explicar cualquier modelo aproximando su límite de decisión o salida de una manera interpretable. Alcance: Global vs Local Explicaciones Locales: Estas se centran en predicciones o decisiones individuales, proporcionando insights sobre por qué el modelo tomó una elección particular para una instancia específica. Herramientas como SHAP y LIME ofrecen explicaciones locales y muestran la influencia de varias características en un único resultado. Explicaciones Globales: En contraste, las explicaciones globales buscan mostrar el comportamiento del modelo a través del conjunto de datos completo o en un sentido más general. Técnicas para lograr explicaciones globales incluyen rankings de importancia de características y gráficas de dependencia parcial, que agregan los efectos de las características a través de múltiples instancias para ofrecer una comprensión más amplia de la lógica del modelo. Las Limitaciones de la IA explicable A medida que la IA continúa permeando varios aspectos de nuestras vidas, el imperativo de transparencia y comprensibilidad en los sistemas de IA nunca ha sido más pronunciado. Sin embargo, como en cualquier dominio en rápida evolución, la IA explicable no está exento de desafíos y limitaciones.\nEl tradeoff entre complejidad y rendimiento El tradeoff entre complejidad y rendimiento sugiere que mejorar el rendimiento de los modelos de IA generalmente requiere aumentar su complejidad. Esta suposición subraya la necesidad percibida de equilibrar la complejidad contra el rendimiento. Sin embargo, esta perspectiva merece un examen crítico. La hipótesis del boleto de lotería propone que la eficacia de los modelos grandes podría derivarse de una inicialización óptima de parámetros en lugar de su mero tamaño. Empíricamente, se ha demostrado que modelos más simples pueden alcanzar un rendimiento comparable al de sus contrapartes más complejas.\nDe hecho, la evidencia muestra que modelos con menos parámetros, como Mistral-7B, pueden alcanzar niveles de rendimiento comparables a los de sus contrapartes más complejas, como GPT-3.5, con una fracción de los parámetros (7 mil millones vs. 175 mil millones). Esta observación desafía la necesidad de modelos excesivamente complejos, abogando por un enfoque más medido en el diseño de modelos donde la simplicidad no excluye la efectividad.\nAlgunos investigadores argumentan que los modelos no inherentemente interpretables deberían evitarse por completo en escenarios de alto riesgo. Sin embargo, sigue sin estar claro si esto es factible en todos los contextos sin sacrificar el rendimiento.\nExplicando la Explicación Para una implementación efectiva de las técnicas de IA explicable, es crucial entender quién es el público objetivo de las explicaciones. La IA explicable sirve principalmente a profesionales técnicos, como científicos de datos e ingenieros de aprendizaje automático, que tienen un profundo entendimiento de estadística.\nSin embargo, existe una distinción significativa entre las explicaciones adecuadas para expertos en IA y aquellas para usuarios no técnicos. Los detalles técnicos pueden abrumar a los usuarios no técnicos, llevando a la confusión o malinterpretación. Por ejemplo, alguien con conocimiento limitado de estadística podría interpretar erróneamente los valores SHAP como indicativos de causalidad, en lugar de mera correlación.\nReconocer el conocimiento, objetivos, habilidades y capacidades del usuario final es primordial al usar la IA explicable. Además, la comunicación de las explicaciones de IA debe estar alineada con el nivel de comprensión de la audiencia. Esto es necesario para evitar tanto los riesgos de simplificación excesiva, que pueden llevar a malentendidos, como la ofuscación, que puede alienar y frustrar a los usuarios.\nFidelidad y los Límites de la Explicación Las explicaciones post-hoc de modelos complejos se logran aproximando localmente su comportamiento alrededor de un punto de datos específico utilizando un modelo más simple y transparente. Aunque estas aproximaciones ofrecen insights, no logran capturar completamente el comportamiento del modelo original. Además, los métodos de IA explicable a menudo carecen de métricas para evaluar la calidad de estas aproximaciones, planteando preguntas sobre la fidelidad de tales explicaciones.\nAdicionalmente, a medida que la complejidad de los modelos aumenta anualmente, nuestra capacidad para explicarlos con precisión disminuye. Esta creciente complejidad destaca una pregunta crucial: ¿Existe un límite teórico a nuestra capacidad para hacer transparentes los procesos de toma de decisiones de la IA? Y si tal límite existe, ¿cuán cerca estamos de alcanzarlo?\nConclusión La IA explicable juega un papel crítico en abordar la opacidad de los complejos modelos de aprendizaje automático, permitiendo confiar en las decisiones basadas en IA. La IA facilita identificar sesgos, depurar modelos, cumplir con regulaciones y generar nuevo conocimiento explicando el comportamiento del modelo. Existen numerosos recursos de código abierto disponibles para la implementación fácil y eficiente de IA explicable. No obstante, es importante reconocer las limitaciones y desafíos asociados con IA explicable y favorecer modelos más simples siempre que sea posible.\nRecursos Libros Análisis de Modelos Explicativos Aprendizaje Automático Interpretable Software SHAP LIME DALEX XAITK InterpretML Bibliografía Ebers. Regulating Explainable AI in the European Union. An Overview of the Current Legal Framework(s). (2021)\nFrankle and Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. (2018)\nJiang and Senge. On Two XAI Cultures: A Case Study of Non-technical Explanations in Deployed AI System. (2021)\nGunning et al. DARPA\u0026rsquo;s explainable AI (XAI) program: A retrospective. (2021)\nLarson et al. How We Analyzed the COMPAS Recidivism Algorithm. (2016)\nLinardatos et al. Explainable AI: A Review of Machine Learning Interpretability Methods. (2020)\nLungberg and Lee. A Unified Approach to Interpreting Model Predictions. (2017)\nNg et al. The benefits and pitfalls of machine learning for biomarker discovery. (2023)\nObermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. (2019)\nRibeiro et al. \u0026ldquo;Why Should I Trust You?\u0026rdquo;: Explaining the Predictions of Any Classifier. (2016)\nRudin. Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead. (2019)\nSarkar. Is explainable AI a race against model complexity? (2022)\nSevilla. Parameter counts in Machine Learning. (2021)\n","permalink":"https://omfmartin.com/es/posts/2024-02-18-explainable-ai/","summary":"Abriendo la caja negra de la IA para entender cómo razonan los algoritmos y fomentar la confianza.","title":"Una Guía Rápida sobre la IA Explicable"},{"content":" Soy un científico de datos senior especializado en el sector de la salud en NTT DATA en Barcelona. Actualmente estoy desarrollando módulos de análisis predictivo para una plataforma de ensayos clínicos. Además, he implementado un dashboard con aprendizaje automático para ayudar a los trabajadores sociales a anticipar la institucionalización en personas mayores. También participé en el Hackathon de IA SmartNation en Bélgica, centrándome en la estructuración de datos clínicos de texto libre utilizando large language models.\nMi camino hacia la ciencia de datos comenzó de una manera poco convencional. Inicialmente estudiando para ser farmacéutico, mi curiosidad me llevó hacia la bioinformática estructural, la estadística y el desarrollo de software. Este cambio me llevó a realizar un Doctorado en Bioinformática. A lo largo de mi trabajo doctoral, exploré el impacto del envejecimiento en el transcriptoma de Caenorhabditis elegans, empleando varias técnicas como análisis de grafos, modelado de series temporales bayesianas, aprendizaje automático y modelos lineales generalizados.\nDurante mi carrera, también apliqué aprendizaje automático para predecir respuestas adversas a la quimioterapia utilizando marcadores genéticos, realicé modelado metabólico cinético y llevé a cabo simulaciones de farmacocinética.\nAparte de mis objetivos profesionales, tengo una pasión por los idiomas. Hablo con fluidez francés, portugués, inglés, español y catalán, y también puedo mantener una conversación en occitano y ruso.\nSoftware zebu: Paquete de R para estimar medidas de asociación local como residuos de chi-cuadrado, D de Lewontin, información mutua puntual y Z de Ducher. También incorpora pruebas de permutación para la evaluación de significancia estadística escritas en C++. Disponible en CRAN con una detallada vignette para explicar su uso.\ntrobalècte: Actualmente estoy desarrollando un algoritmo en Python para detectar dialectos occitanos a partir de muestras de texto utilizando NLP.\nisotela: Paquete de R para la normalización y análisis diferencial de datos de RNA-seq de individuos únicos, teniendo en cuenta las diferencias en el tamaño del tejido. Requiere de spike-ins.\npoète-mécanique: Lee un poema al azar en voz alta cuando el sensor de movimiento de Raspberry Pi detecta movimiento.\nPublicaciones revisadas por pares Systematic mapping of organism-scale gene-regulatory networks in aging using population asynchrony. Eder, Matthias, Olivier MF Martin, Natasha Oswal, Lucia Sedlackova, Cátia Moutinho, Andrea Del Carmen-Fabregat, Simon Menendez Bravo, Arnau Sebé-Pedrós, Holger Heyn, Nicholas Stroustrup. Cell (2024).\nThe unusual kinetics of lactate dehydrogenase of Schistosoma mansoni and their role in the rapid metabolic switch after penetration of the mammalian host. Bexkens, Michiel L, Olivier MF Martin, Jos M van den Heuvel, Marion GJ Schmitz, Bas Teusink, Barbara M Bakker, Jaap J van Hellemond, Jurgen R Haanstra, Malcolm D Walkinshaw, Aloysius GM Tielens. International Journal for Parasitology (2024)\nNeuronal Temperature Perception Induces Specific Defenses That Enable C. elegans to Cope with the Enhanced Reactivity of Hydrogen Peroxide at High Temperature. Servello, Francesco A, Rute Fernandes, Matthias Eder, Nathan Harris, Olivier MF Martin, Natasha Oswal, Anders Lindberg, et al. eLife 11 (2022)\nA Hierarchical Process Model Links Behavioral Aging and Lifespan in C. elegans.Oswal, Natasha, Olivier MF Martin, Sofia Stroustrup, Monika Anna Matusiak Bruckner, and Nicholas Stroustrup. PLOS Computational Biology 18, no. 9 (2022)\nCaenorhabditis elegans Processes Sensory Information to Choose between Freeloading and Self-Defense Strategies. Schiffer, Jodie A, Francesco A Servello, William R Heath, Francis Raj Gandhi Amrit, Stephanie V Stumbur, Matthias Eder, Olivier MF Martin, et al. eLife 9 (2020)\nImplication of Terminal Residues at Protein-Protein and Protein-DNA Interfaces. Martin, Olivier MF, Loïc Etheve, Guillaume Launay, and Juliette Martin. PLOS ONE 11, no. 9 (2016).\nAre Ciprofloxacin Dosage Regimens Adequate for Antimicrobial Efficacy and Prevention of Resistance? Pseudomonas aeruginosa Bloodstream Infection in Elderly Patients as a Simulation Case Study. Cazaubon, Yoann, Laurent Bourguignon, Sylvain Goutelle, Olivier MF Martin, Pascal Maire, and Michel Ducher. Fundamental \u0026amp; Clinical Pharmacology 29, no. 6 (2015)\n","permalink":"https://omfmartin.com/es/sobre-mi/","summary":"Soy un científico de datos senior especializado en el sector de la salud en NTT DATA en Barcelona. Actualmente estoy desarrollando módulos de análisis predictivo para una plataforma de ensayos clínicos. Además, he implementado un dashboard con aprendizaje automático para ayudar a los trabajadores sociales a anticipar la institucionalización en personas mayores. También participé en el Hackathon de IA SmartNation en Bélgica, centrándome en la estructuración de datos clínicos de texto libre utilizando large language models.","title":"Sobre Mi"}]